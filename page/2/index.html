<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="语料库、数据集及工具资源和教程">
<meta property="og:type" content="website">
<meta property="og:title" content="世界语言资源平台">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="世界语言资源平台">
<meta property="og:description" content="语料库、数据集及工具资源和教程">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="世界语言资源平台">
<meta name="twitter:description" content="语料库、数据集及工具资源和教程">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/"/>





  <title>世界语言资源平台</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">世界语言资源平台</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/03/朱述承_内阁大库档案/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/03/朱述承_内阁大库档案/" itemprop="url">内阁大库档案</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-03T15:37:46+05:00">
                2018-06-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：朱述承<br>访问地址：<a href="http://archive.ihp.sinica.edu.tw/mctkm2/index.html" target="_blank" rel="noopener">http://archive.ihp.sinica.edu.tw/mctkm2/index.html</a></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>内阁大库档案原藏于清宫内阁大库，宣统元年(1909)因大库整修而被移出。清亡后几经转手，一度被卖入同懋增纸厂作还魂纸，最后在首任所长傅斯年先生的奔走下，于民国十八年(1929)自李盛铎手中购入。这批档案有四千多件明代(1368-1644)文书，三十多万件清代(1644-1911)档册，包括内阁收贮的制诏诰敕、题奏本章、朝贡国表章、内阁各厅房处的档案、修书各馆档案、试题、试卷、渖阳旧档等，而以题奏本章佔最大宗。内阁大库档案内容多涉及一般行政事务，而许多案例并不见于会典或则例，是研究制度史的重要材料，同时对于社会史、经济史或法制史等的研究也极具价值。 </p>
<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>分为“免费使用”和“授权使用”两种使用方式。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/03/杜成玉_Antconc3.21语料库分析统计软件/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/03/杜成玉_Antconc3.21语料库分析统计软件/" itemprop="url">Antconc3.21语料库分析统计软件</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-03T15:37:46+05:00">
                2018-06-03
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：杜成玉<br>下载地址：<a href="http://www.laurenceanthony.net/software/antconc/" target="_blank" rel="noopener">http://www.laurenceanthony.net/software/antconc/</a></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>该语料库工具是语料库检索软件，具有以下特点：<br>（1）能识别txt，html，htm，xml这四种格式；<br>（2）可以统计出关键词在文本中出现的次数；<br>（3）能列出文本中的词项数和词形，还能将词项数按出现的频率高低排列；<br>（4）文本去重；<br>（5）能够将某个词的搭配按照统计数据从高到低或者反向排序。<br>（6）模糊检索</p>
<h1 id="使用教程："><a href="#使用教程：" class="headerlink" title="使用教程："></a>使用教程：</h1><p>（1）从file菜单的openfile（打开文件）或opendir（打开目录）选择一个或多个要处理的文件，选出来的文件按顺序在主窗户的左边框里显示出来。<br>（2）在左边的按钮条的输入框里输入一个检索词<br>（3）使用右边”SearchWindowSize”（检索窗口大小）的按钮条的增加和减少按钮来选择在检索词两边显示的字符数。<br>（4）按“Start”（开始）键开始产生索引行的检索结果。检索过程中可按“stop”(停止）键随时停止检索。<br>（5）使用KwicSort（上下文关键词分类）下的按钮条选择一个目标词来重排索引行,0是检索词，1L，2L是检索词左边的第一，第二个单词，1R，2R是检索词右边第一，第二个单词。<br>（6）按“Sort”（分类）键开始分类处理。<br>（7）将指针移到其中一行索引行的突出的检索词之上，系统默认为蓝色。指针会转变成一个手形的图标。点击突出的检索词，可以看到检索词在原文中出现的情况。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/卢梦依_MS MARCO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/卢梦依_MS MARCO/" itemprop="url">MS MARCO</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T21:28:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>MS MARCO是一种新的大规模阅读理解和问答数据集。 在MS MARCO中，所有问题都是从真正的匿名用户查询中抽取的。使用先进的Bing搜索引擎版本，从实际的Web文档中提取数据集中的答案的上下文段落。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p><a href="http://www.msmarco.org/" target="_blank" rel="noopener">http://www.msmarco.org/</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]. In this issue[J]. European Journal of Immunology,2010,40(5).</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/卢梦依_SVHN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/卢梦依_SVHN/" itemprop="url">SVHN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T21:23:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>SVHN数据来源于 Google 街景视图中房屋信息，它是一个真实世界的图像数据集，用于开发机器学习和对象识别算法，对数据预处理和格式化的要求最低。它跟MNIST相似，但是包含更多数量级的标签数据（超过60万个数字图像），并且来源更加多样，用来识别自然场景图像中的数字。</p>
<h1 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h1><p><a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" rel="noopener">http://ufldl.stanford.edu/housenumbers/</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]Shuai Li,Wenfeng Song,Hong Qin,Aimin Hao. Deep variance network: An iterative, improved CNN framework for unbalanced training datasets[J]. Pattern Recognition,2018,81.<br>[2]Andrey V. Savchenko,Natalya S. Belova. Unconstrained face identification using maximum likelihood of distances between deep off-the-shelf features[J]. Expert Systems With Applications,2018,108.<br>[3]Alistair Peter McGeorge. An Urban Partnership for Inner Sydney Social Inclusion, Health and Well-being[J]. International Journal of Integrated Care,2017,17(3).</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/卢梦依_Labeled Faces in the Wild数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/卢梦依_Labeled Faces in the Wild数据集/" itemprop="url">Labeled Faces in the Wild数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T21:19:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>该数据集是用于研究无约束面部识别问题的面部照片数据库。数据集包含从网络收集的13000多张图像。每张脸都贴上了所画的人的名字，图片中的1680人在数据集中有两个或更多不同的照片。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p><a href="http://vis-www.cs.umass.edu/lfw/" target="_blank" rel="noopener">http://vis-www.cs.umass.edu/lfw/</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]David Rim,Md Kamrul Hasan,Fannie Puech,Christopher J. Pal. Learning from weakly labeled faces and video in the wild[J]. Pattern Recognition,2015,48(3).<br>[2]Davide Lombardo. An explicit open image theorem for products of elliptic curves[J]. Journal of Number Theory,2016,168.<br>[3]M. Nazir,A. Majid-Mirza,S. Ali-Khan. PSO-GA Based Optimized Feature Selection Using Facial and Clothing Information for Gender Classification[J]. Journal of Applied Research and Technology,2014,12(1).<br>[4]Jiang-Jing Lv,Cheng Cheng,Guo-Dong Tian,Xiang-Dong Zhou,Xi Zhou. Landmark perturbation-based data augmentation for unconstrained face recognition[J]. Signal Processing: Image Communication,2016,47.<br>[5]Blondin , John M.,Kallman , Timothy R.,Pereyra , Nicolas Antonio. Hydrodynamic Models of Line-Driven Accretion Disk Winds in Cataclysmic Variables[J]. Revista Mexicana de Astronomía y Astrofísica : Universidad Nacional Autónoma de México. Instituto de Astronomía,2001(11).<br>[6]Ian W. Roxburgh. Challenges to Theories of the Structure of Moderate-Mass Stars[M].Springer Berlin Heidelberg:2005-07-19.<br>[7]Andrey V. Savchenko,Natalya S. Belova. Unconstrained face identification using maximum likelihood of distances between deep off-the-shelf features[J]. Expert Systems With Applications,2018,108. </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/卢梦依_LSUN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/卢梦依_LSUN/" itemprop="url">LSUN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T21:14:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>国外的PASCAL VOC和ImageNet ILSVRC比赛使用的数据集，数据领域包括卧室、冰箱、教师、厨房、起居室、酒店等多个主题。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p><a href="http://lsun.cs.princeton.edu/2017/" target="_blank" rel="noopener">http://lsun.cs.princeton.edu/2017/</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]Blondin , John M.,Kallman , Timothy R.,Pereyra , Nicolas Antonio. Hydrodynamic Models of Line-Driven Accretion Disk Winds in Cataclysmic Variables[J]. Revista Mexicana de Astronomía y Astrofísica : Universidad Nacional Autónoma de México. Instituto de Astronomía,2001(11).<br>[2]Ian W. Roxburgh. Challenges to Theories of the Structure of Moderate-Mass Stars[M].Springer Berlin Heidelberg:2005-07-19.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘唯_Labelme/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘唯_Labelme/" itemprop="url">Labelme</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T20:10:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘唯</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p> Labelme是斯坦福一个学生的母亲利用休息时间帮儿子做的标注，后来便发展为一个数据集。该数据集的主要特点包括<br>（1）专门为物体分类识别设计，而非仅仅是实例识别<br>（2）专门为学习嵌入在一个场景中的对象而设计<br>（3）高质量的像素级别标注，包括多边形框（polygons）和背景标注（segmentation masks）<br>（4）物体类别多样性大，每种物体的差异性，多样性也大。<br>（5）所有图像都是自己通过相机拍摄，而非copy<br>（6）公开的，免费的</p>
<h1 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h1><p>图像如下图所示，需要通过matlab来下载，一种奇特的下载方式。<br>下载链接为<a href="http://labelme2.csail.mit.edu/Release3.0/index.php" target="_blank" rel="noopener">http://labelme2.csail.mit.edu/Release3.0/index.php</a><br><img src="https://i.loli.net/2018/05/24/5b06ac87bd99e.jpg" alt=""></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]吉江燕,方挺.基于Labelme的参考图像的手工分割[J].微型机与应用,2015,34(17):49-51+56.<br>[2]Bryan C. Russell,  Antonio Torralba,  Kevin P. Murphy,  William T. Freeman.International Journal of Computer Vision[J].LabelMe: A Database and Web-Based Tool for Image Annotation.<br>[3]</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘唯_Open Image/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘唯_Open Image/" itemprop="url">Open Image</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T20:10:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘唯</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>过去几年机器学习的发展使得计算机视觉有了快速的进步，系统能够自动描述图片，对共享的图片创造自然语言回应。其中大部分的进展都可归因于 ImageNet 、COCO这样的数据集的公开使用。谷歌作为一家伟大的公司，自然也要做出些表示，于是乎就有了Open Image。</p>
<p>Open Image是一个包含~900万张图像URL的数据集，里面的图片通过标签注释被分为6000多类。该数据集中的标签要比ImageNet（1000类）包含更真实生活的实体存在，它足够让我们从头开始训练深度神经网络。</p>
<p>谷歌出品，必属精品！唯一不足的可能就是它只是提供图片URL，使用起来可能不如直接提供图片方便。</p>
<h1 id="数据集大小"><a href="#数据集大小" class="headerlink" title="数据集大小"></a>数据集大小</h1><p>~1.5GB（不包括图片）</p>
<h1 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h1><p><a href="https://github.com/openimages/dataset" target="_blank" rel="noopener">https://github.com/openimages/dataset</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]M. Hu&amp;scaron,ek. Open images of orderable spaces[J]. proc,1983,88(4).<br>[2]Davide Lombardo. An explicit open image theorem for products of elliptic curves[J]. Journal of Number Theory,2016,168.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_Moments-in-Time/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_Moments-in-Time/" itemprop="url">Moments in Time</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//moments.csail.mit.edu/" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//moments.csail.mit.edu/</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>在过去一年中，视频理解相关的领域涌现了大量的新模型、新方法，与之相伴的，今年也出现了多个新的大规模的视频理解数据集。近期，MIT-IBM Watson AI Lab 就推出了一个全新的百万规模视频理解数据集Moments-in-Time虽然没有之前的YouTube-8M数据集大，但应该是目前多样性，差异性最高的数据集了。该数据集的任务仍然为视频分类任务，不过其更专注于对“动作”的分类，此处的动作为广义的动作或动态，其执行者不一定是人，也可以是物体或者动物，这点应该是该数据集与现有数据集最大的区分。本文中简单的统称为“动作”。</p>
<h1 id="数据集概览"><a href="#数据集概览" class="headerlink" title="数据集概览"></a>数据集概览</h1><p>这部分主要对数据集的基本情况和特性进行介绍，大概可以总结为以下几点：</p>
<ul>
<li>共有100,0000个视频，每个视频的长度相同，均为3s</li>
<li>每个视频有一个动作标签（后续版本可能拓展为多标签），此处的动作仅为动词，比如“opening”就为一个标签（与之不同，其他数据集经常会采用动名词组的形式如”opening the door”）</li>
<li>动作主体可以是人，动物，物体乃至自然现象。</li>
<li>数据集的类内差异和类间差异均很大。</li>
<li>存在部分或完全依赖于声音信息的动作，如clapping（拍手）<br>由上述描述可以看出，由于超大的数据量以及多样性，这个数据集是相当难的，下图则为该数据集的一个例子。可以看出，一个动作类别可以由多种动作主体完成，从而从视觉上看的差异性相当的大，动作的概念可以说是相当抽象了。<br><img src="https://i.loli.net/2018/06/02/5b1212dc40bc6.jpg" alt=""></li>
</ul>
<p>下面我对作者构建这个数据集的方式进行介绍，这部分内容也有助于对该数据集的理解。</p>
<h1 id="数据集的构建"><a href="#数据集的构建" class="headerlink" title="数据集的构建"></a>数据集的构建</h1><h2 id="建立动作的字典"><a href="#建立动作的字典" class="headerlink" title="建立动作的字典"></a>建立动作的字典</h2><p>该数据集采用的是先确定动作标签，再根据动作标签构建视频集合的方式。构建动作标签集合，在该数据集中即构建一个合适的动作字典。主要通过以下几个步骤实现：</p>
<ul>
<li>参考[2]中的内容，选取4500个美式英语中最常用的动词</li>
<li>按照词义对这4500个词进行聚类，一个动词可以属于多个聚类</li>
<li>迭代的从最常见的聚类中选取最常见的动词加入目标字典</li>
<li>最终从4500个初始动词中选取339个最常见的动词作为字典  </li>
</ul>
<p><img src="https://i.loli.net/2018/06/02/5b1212f219cc3.jpg" alt=""></p>
<h2 id="数据收集与标注"><a href="#数据收集与标注" class="headerlink" title="数据收集与标注"></a>数据收集与标注</h2><p>在确定好动词字典后，作者对每个动词，在多个视频网站上进行视频的爬取。这里的视频网站比较多，包含YouTube，Flicker，Vine等十几个网站，比起只用YouTube的ActivityNet，Kinectic等数据集在来源的丰富性上要高不少。</p>
<p>在爬完数据后，每个视频都是以 视频-动词 对的形式呈现，标注工作的主要目的就是确定视频是否可以用动词描述，所以是一个二分类的标注任务（此处作者的解释是，多分类的标注对于标注者难度太高，也容易错，故采用二分类的标注方式）。标注工作在近来大量数据集都采用的Amazon Mechanical Turk实现。</p>
<p>对于每个标注者，都会被分配64个待标注的动词-视频对以及10个已知真值的动词-视频对。在10个已知真值的动词-视频对中，只有标对9个及以上，该标注者的标注结果才会被认为是有效的。剩下的所有动词-视频对，都会被交由2个标注者，只有俩人的标注结果一致，该结果才会被采用。所以从标注角度来看，这个数据集的标签质量应该还是不错的。标注界面的样式如下图所示，可以看出还是相当简洁明了的。<br><img src="https://i.loli.net/2018/06/02/5b121a603d679.jpg" alt=""></p>
<h1 id="数据集的数据分布"><a href="#数据集的数据分布" class="headerlink" title="数据集的数据分布"></a>数据集的数据分布</h1><p>接下来我主要对该数据集的数据分布进行介绍，由于该数据集目前还没有正式放出，所以所有数据和图表均来自论文。</p>
<p>首先是数据集的类别分布：</p>
<ul>
<li>对于339个动作类别，共有超过100000个标注视频</li>
<li>每个类别至少有1000个视频，每个类别视频数量的平均值是1757，中值是2775</li>
</ul>
<p>类别与类别视频数量的关系图如下图所示。  </p>
<p><img src="https://i.loli.net/2018/06/02/5b12127aa3e93.jpg" alt=""></p>
<p>接下来，作者介绍了数据集中动作主体的分布情况，如前所述动作主题可能是人，动物或一般物体。作者统计了不同类别视频中各类动作主题所占比例的分布，如下图所示。左侧的极端是“typing“，主体全部是人类，右边的极端是”overflowing”,动作主题基本不是人类。<br><img src="https://i.loli.net/2018/06/02/5b121295f1aba.jpg" alt=""></p>
<p>最后，作者分析了数据集各个类别中依赖于声音的视频所在的比例。此处，依赖于声音的视频是指该视频无法从图像上判断出其包含的动作，而必须要听声音。从下图可以看出，有相当比例的视频是依赖于声音的，这点要增加了该数据集的挑战性。</p>
<p><img src="https://i.loli.net/2018/06/02/5b1212a55102b.jpg" alt=""></p>
<h1 id="场景、物体与动作之前的相关性探索"><a href="#场景、物体与动作之前的相关性探索" class="headerlink" title="场景、物体与动作之前的相关性探索"></a>场景、物体与动作之前的相关性探索</h1><p>最后，作者通过一组简单的实验探索了各个数据集中 物体-场景-动作 之间的相关性。此处分析的视频数据集除了Moments in Time外， 还包括UCF-101, ActivityNet 1.3 以及Kinetics数据集。</p>
<p>这里的实验设置还蛮有趣的。作者分别采用了一个在ImageNet上训练的Resnet50用于物体分类，一个在Places数据集上训练的Resnet50用作场景分类。对于每个视频，均匀抽取3帧并利用两个网络进行检测并平均结果，可以得到一个物体label以及一个场景label。对于物体或场景label，作者通过贝叶斯公式来推断对应的动作类别，其中先验概率在数据集的训练集上计算获得。</p>
<p>实验结果如下表所示，可以得到以下几点结论：</p>
<ul>
<li>动作与场景以及物体均是相关的。</li>
<li>Moments in Time数据集中，动作与物体以及场景的相关性显著弱于其他几个数据集，这表明该数据集有更高的挑战性以及更大的难度。<br><img src="https://i.loli.net/2018/06/02/5b1212be03883.jpg" alt=""></li>
</ul>
<h1 id="个人讨论"><a href="#个人讨论" class="headerlink" title="个人讨论"></a>个人讨论</h1><p>Moments-in-Time数据集我觉得还是相当有趣以及有挑战性的，估计很快就会有不少人跟进来做这个数据集（显而易见需要比较大的计算资源…）。下面是我对于该数据集的一些讨论内容，包括优点以及一些个人存在疑惑的地方。</p>
<p><strong>优点：</strong></p>
<ul>
<li>数据集的大小和丰富程度很高，足以训练较复杂的视频分类模型。</li>
<li>视频的长度统一为3s，这样的设计方便实验时进行处理，也使得数据集的尺寸不至于过大。</li>
<li>数据标注的策略应该还是比较靠谱的，应该不太会有错误标注。</li>
</ul>
<p>以上是几点明显的优点，但对于作者强调的几个数据集优点，我则存在一些疑惑：</p>
<ul>
<li>仅用动词定义动作：这个应该是这个数据集和其他数据集相比最大的一个差异点。作者认为通过该数据集能够学习一个泛化能力很强的动作概念，但在我看来这样的定义有些太过宽泛了。动词的含义常常依赖于其主语和谓语，单独的动词即便对于人类而言也常常是含义模糊的。此处可以参考今年ICCV上的[3]一文，我此前也写过一篇笔记：<a href="https://zhuanlan.zhihu.com/p/29227174" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29227174</a> 介绍这篇文章。这篇文章中一个重要的观点是，动作应该用动词-名词组合来定义，从而明确其含义。不过该数据集也是故意在此处模糊化从而增加类内差异，现在也不能够知道是否是一个好的设计了。</li>
<li>动作的主体不一定是人：这点也是数据集作者有意设计，从而增加难度以及多样性。我也持有同样的对于定义不清晰的疑惑，比如人开门（“opening”）和风吹开了一扇窗户（”opening“）放在同一个类别中总感觉不太合理。此外，此处还有一个问题，尽管温中给出了动作主体的分析，但通过询问作者，第一版的数据集不会提供动作主体的label，而仅包含一个动作label。</li>
<li>依赖声音的动作：这点我觉得倒是蛮好的，可以促进多模态方法的发展。但是同以上一点，该数据集在训练集中并没有告知这个视频中的动作是否是依赖与声音的。如果有相关的标签，我觉得会更有助于视频的理解吧。作者可能会在后续版本加上。<br>总体而言，这个新数据集还是很有趣且充满挑战的，与此前的多个主要关注人类动作的数据集在设定上有较大的差异。针对这个数据集，模型方面应该更注重于对动作概念的理解以及对较大的类内差异性的处理。期待之后针对该数据集的算法了。</li>
</ul>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Monfort M, Zhou B, Bargal S A, et al. Moments in Time Dataset: one million videos for<br>event understanding[J].</p>
<p>[2] Salamon J, Jacoby C, Bello J P. A dataset and taxonomy for urban sound research[C]//Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014: 1041-1044.</p>
<p>[3] Sigurdsson G A, Russakovsky O, Gupta A. What Actions are Needed for Understanding<br>Human Actions in Videos?[J]. arXiv preprint arXiv:1708.02696, 2017.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_UCF101 - Action Recognition Data Set/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_UCF101 - Action Recognition Data Set/" itemprop="url">UCF101 - Action Recognition Data Set</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://crcv.ucf.edu/data/UCF101.php" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101.php</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>UCF101是一个动作识别数据集，包含现实的动作视频，从YouTube上收集，有101个动作类别。该数据集是UCF50数据集的扩展，该数据集有50个动作类别。<br>从101个动作类的13320个视频中，UCF101给出了最大的多样性，并且在摄像机运动、物体外观和姿态、物体尺度、视点、杂乱背景、光照条件等方面存在较大的差异，这是迄今为止最具挑战性的数据。<br>由于大多数可用的动作识别数据集都不现实，而且是由参与者进行的，UCF101旨在通过学习和探索新的现实行动类别来鼓励进一步研究行动识别。<br>101个动作类的视频被分成25组，每个组可以包含4-7个动作视频。同一组的视频可能有一些共同的特点，比如背景相似、观点相似等。  </p>
<p><strong>动作类别可以分为五类： </strong> </p>
<ul>
<li>Human-Object Interaction  </li>
<li>Body-Motion Only   </li>
<li>Human-Human Interaction   </li>
<li>Playing Musical Instruments   </li>
<li>Sports.   </li>
</ul>
<p>UCF101数据集的操作类别为:应用眼妆、唇膏、射箭、婴儿爬行,平衡木,乐队游行,棒球,篮球,篮球扣篮,卧推,骑自行车,台球,吹干头发,吹蜡烛,体重下蹲,保龄球,拳击出气筒,拳击袋速度,蛙泳,刷牙,挺举,悬崖跳水,板球保龄球,板球,削减在厨房,潜水,打鼓,击剑、曲棍球点球,地板体操,飞盘,爬泳,高尔夫挥杆,发型、链球、锤击,倒立俯卧撑,倒立行走,头部按摩,跳高,赛马,骑马、呼啦圈、冰上舞蹈,掷标枪,杂耍球,跳绳,跳杰克,皮划艇,针织,跳远,弓步,阅兵,搅拌面糊、拖地板,修女轻叩,双杠,披萨扔,弹吉他,弹钢琴,打手鼓,演奏小提琴,演奏大提琴,玩来说,玩时代,演奏长笛,玩锡塔尔琴,撑杆跳,鞍马、拉Ups、穿孔、俯卧撑、漂流、攀岩室内,绳索攀爬、划船、萨尔萨舞旋转,剃胡子,推铅球,滑板,滑雪,Skijet,跳伞,足球杂耍,足球点球,还是戒指,相扑,冲浪,秋千,乒乓球拍、太极、网球挥拍,扔铁饼,蹦床跳,打字,高低杠,排球飙升,与狗一起散步,“推墙”,写作,溜溜球。  </p>
<p>下载UCF101数据集：<a href="http://crcv.ucf.edu/data/UCF101/UCF101.rar" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101.rar</a>  </p>
<p>UCF101数据集的动作识别（ Action Recognition）的训练/测试集下载地址:<a href="http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip</a>  </p>
<p>UCF101数据集的动作检测（ Action Detection）的训练/测试集下载地址:<a href="http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-DetectionTask.zip" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-DetectionTask.zip</a>  </p>
<p>UCF101数据集的STIP特性可以在这里下载:<a href="http://crcv.ucf.edu/data/UCF101/UCF101_STIP_Part1.rar" target="_blank" rel="noopener">Part1</a>,<a href="http://crcv.ucf.edu/data/UCF101/UCF101_STIP_Part2.rar" target="_blank" rel="noopener">Part2</a>  </p>
<p><img src="https://i.loli.net/2018/06/02/5b121da3d8ad6.jpg" alt="">  </p>
<h1 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h1><p><img src="http://crcv.ucf.edu/data/UCF101/Clip%20Duration%201.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Clip%20Duration%202.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Number%20of%20Videos%201.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Number%20of%20Videos%202.jpg" alt="">  </p>
<p>注意：将属于同一组的视频保持在训练和测试中非常重要。由于一组中的视频是从单个长视频中获得的，因此在训练和测试套件中共享来自同一组的视频会获得较高的性能。 </p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Khurram Soomro, Amir Roshan Zamir and Mubarak Shah, UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild, CRCV-TR-12-01, November, 2012.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">CNLR</p>
              <p class="site-description motion-element" itemprop="description">语料库、数据集及工具资源和教程</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">164</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CNLR</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
