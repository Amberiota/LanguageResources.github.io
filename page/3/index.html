<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="语料库、数据集及工具资源和教程">
<meta property="og:type" content="website">
<meta property="og:title" content="世界语言资源平台">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="世界语言资源平台">
<meta property="og:description" content="语料库、数据集及工具资源和教程">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="世界语言资源平台">
<meta name="twitter:description" content="语料库、数据集及工具资源和教程">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/3/"/>





  <title>世界语言资源平台</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">世界语言资源平台</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_百度BROAD-Video Highlights视频精彩片段数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_百度BROAD-Video Highlights视频精彩片段数据集/" itemprop="url">百度BROAD-Video Highlights视频精彩片段数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://ai.baidu.com/broad" target="_blank" rel="noopener">http://ai.baidu.com/broad</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>该数据集（下文中简称为BROAD-VH数据集）在介绍中将其定义为视频精彩片段提取任务。具体而言，就是提取视频中可能包含精彩片段的时间区域，而不需要对精彩片段的类别进行分类。该任务实际上与我之前介绍过的temporal action proposal 任务（相关介绍和算法可以参考<a href="https://zhuanlan.zhihu.com/p/31501316" target="_blank" rel="noopener">Temporal Action Detection (时序动作检测)方向2017年会议论文整理）</a>是完全相同的。</p>
<h1 id="视频及数据信息"><a href="#视频及数据信息" class="headerlink" title="视频及数据信息"></a>视频及数据信息</h1><p>BROAD-VH数据集主要来源于爱奇艺视频。视频类型为综艺节目，目前包括1500个长视频，视频总时长约1200小时。该数据集的视频时长分布图如下图所示（长度单位为帧）。按照总帧数和总时长的比例，估计采样的fps大概为1.5吧，算是比较低的采样频率了。</p>
<p>训练/验证/测试集的视频数量划分为1262/120/117。</p>
<p>该数据集通过爱奇艺网页link的方式提供了原始视频（即需要爬虫下载或手动下载），此外还提供了提取好的image feature和audio feature。这两种特征均在视频的每一帧上提取，维度均为2048。比如对于一个长度为1000帧的视频，image和audio特征矩阵的大小均为1000*2048。</p>
<h1 id="标签信息及分布"><a href="#标签信息及分布" class="headerlink" title="标签信息及分布"></a>标签信息及分布</h1><p>该数据集中一共有18000个精彩片段的时序标注，平均一个视频有12个时序标注。这些精彩片段的总时长占1500个小时中的750个小时，即有一半左右的视频时长被标注为了精彩片段。</p>
<p>我对训练集的标签信息进行了分析，分析的主要内容为精彩片段时长的分布，分布直方图如下图所示。</p>
<p>可以看出，大部分精彩片段的长度都在30-300帧的范围。</p>
<h1 id="测评方式"><a href="#测评方式" class="headerlink" title="测评方式"></a>测评方式</h1><p>测评方式部分与通常temporal action proposal任务中不同，并没有使用average recall (平均召回率），而是同detection任务一样使用了mAP，此处将所有highlights片段都看作为了一个动作类别。比较有趣的是，BROAD-VH基本上直接使用了ActivityNet Challenge的detection任务测评代码（略有改动）。</p>
<h1 id="简要分析"><a href="#简要分析" class="headerlink" title="简要分析"></a>简要分析</h1><p>根据上面的介绍以及分析内容，可以对这个数据集进行一些简单的评价：</p>
<ul>
<li>单个视频的时长可能很长（小时级别），单个视频中包含的精彩片段也比较多，这点与THUMOS数据集很像，而与单个视频时长短且包含片段少的ActivityNet数据集差异大</li>
<li>数据集标注的格式，测评代码等方面应该是直接参考的ActivityNet 数据集做的</li>
<li>数据的规模还是比较大的，从时长方面看比ActivityNet要长（ActivityNet时长大约为700小时）</li>
<li>视频的来源均为综艺视频，这点表明这个数据集的来源多样性比较单一</li>
<li>提供特征，其目的应该是节省研究者的计算开销。估计1500小时的视频，提取一遍特需要很长的时间。。根本没法玩。所以有现成的特征挺不错的。</li>
</ul>
<h1 id="简单的尝试"><a href="#简单的尝试" class="headerlink" title="简单的尝试"></a>简单的尝试</h1><p>下完数据集我就先跑了一个最简单的baseline方法，即activitynet challenge 2017 proposal task中的baseline：uniform random 方法。代码主要参考了activitynet官方提供的代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/activitynet/ActivityNet/blob/master/Notebooks/ActivityNet-Release1.3.Proposals.ipynb" target="_blank" rel="noopener">activitynet/proposals</a></p>
<p>简单而言，就是在视频随机的位置产生随机长度的proposals，并给予随机的confidence score。在验证集中，对于每个视频我生成了200个proposals，得到的mAP大概在0.027 左右。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_CoPhIR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_CoPhIR/" itemprop="url">CoPhIR 数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓<br>地址：<a href="http://cophir.isti.cnr.it/whatis.html" target="_blank" rel="noopener">http://cophir.isti.cnr.it/whatis.html</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>雅虎发布的超大Flickr数据集，包含1亿多张图片。<br>CoPhIR（Content-based Photo Image Retrieval，基于内容的照片图像检索）测试集合的开发旨在对SAPIR项目基础设施（SAPIR：使用对等IR中的音频视频内容进行搜索）的可扩展性进行重要测试以进行相似性搜索。 CoPhIR是NMIS实验室与意大利比萨ISTI-CNR的HPC实验室共同努力的成果。 我们通过DILIGENT项目使用EGEE European GRID从Flickr存档提取元数据。 对于每个图像，已经提取了标准的MPEG-7图像特征。试验台的每个入口都包含：  </p>
<ul>
<li>链接Flickr网站的相应条目   </li>
<li>照片图像缩略图  </li>
<li>一个XML结构，在相应的Flickr条目中包含Flickr用户信息：标题，位置，GPS，标签，注释等。   </li>
<li><p>具有5个提取的标准MPEG-7图像特征的XML结构：  </p>
<ul>
<li>可伸缩的色彩  </li>
<li>色彩结构 </li>
<li>色彩布局  </li>
<li>边缘直方图</li>
<li>均匀纹理  </li>
</ul>
</li>
</ul>
<p>迄今收集的数据代表世界上最大的多媒体元数据收集，可用于可扩展相似性搜索技术的研究。 CoPhIR包含1.06亿个处理过的图像。   </p>
<p>CoPhIR现在可供研究人员尝试比较不同的索引技术进行相似性搜索，其中可扩展性是关键问题。  </p>
<p>我们使用Flickr图片内容符合Creative Commons许可。 CoPhIR测试集合符合基于WIPO（世界知识产权组织）版权条约和表演和录音制品条约以及意大利现行法律68/2003的欧洲第29/2001号建议书。<br>为了访问CoPhIR发行版，有兴趣在其上进行实验的组织（大学，研究实验室等）将必须签署随附的CoPhIR访问协议和CoPhIR访问注册表，将原始签名文件通过邮件发送给我们。请按照“如何获得CoPhIR测试集合”一节中的说明进行操作。然后，您将收到登录和密码以下载所需的文件。  </p>
<h1 id="使用–获得CoPhIR测试集"><a href="#使用–获得CoPhIR测试集" class="headerlink" title="使用–获得CoPhIR测试集"></a>使用–获得CoPhIR测试集</h1><ul>
<li>发送电子邮件到 cophiristi.cnr.it (subject: new access to Cophir)，包含有必要信息的请求(见<a href="http://cophir.isti.cnr.it/RequestTemplate.txt" target="_blank" rel="noopener">请求模板</a>)。  </li>
<li>打印CoPhIR Access Agreement和CoPhIR Access Registration Form (doc, pdf)，填写所需信息，然后由授权人签署正本文件。  </li>
<li>将两份文件邮寄至</li>
</ul>
<blockquote>
<p>Dr. Fausto Rabitti<br>NMIS Lab.<br>ISTI-CNR, Pisa Research Area<br>Via Moruzzi, 1<br>56124 Pisa (Italy).  </p>
</blockquote>
<ul>
<li><p>我们将发送到您的电子邮件地址，在访问注册表中显示，一封包含登录名和密码的电子邮件将用于访问CoPhIR测试集合。  </p>
</li>
<li><p>要下载CoPhIR测试集合的文件，请在CoPhIR网站上输入<a href="http://cophir.isti.cnr.it/download.html" target="_blank" rel="noopener">下载</a>部分并使用您的登录名和密码。  </p>
</li>
</ul>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] F Rabitti, R Perego,F Falchi,C Lucchese, P Bolettieri, CoPhIR (Content-based Photo Image Retrieval) Test-Collection, 2008<br>[2] M Batko,P Kohoutkova,D Novak, CoPhIR Image Collection under the Microscope, 2009  </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_Kinetics-600 dataset/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_Kinetics-600 dataset/" itemprop="url">Kinetics-600 dataset</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/" target="_blank" rel="noopener">https://deepmind.com/research/open-source/open-source-datasets/kinetics/</a>  </p>
<h1 id="背景与来源"><a href="#背景与来源" class="headerlink" title="背景与来源"></a>背景与来源</h1><p>目前activitynet第三届比赛已经开始了，这项比赛始于2016CVPR，是与ImageNet齐名的在视频理解方面最重要的比赛。</p>
<p>在这个比赛下的Task A – Trimmed Action Recognition比赛是一个视频分类比赛，数据集就是kinetics-600数据集。数据集有Google的deepmind团队提供，2017年是第一届比赛，当时有400个类别，20多万数据，今年又对数据集进行了扩增，现在有600个类别，共50万左右的视频。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>视频来源于YouTube，一共有600个类别，每个类别至少600个视频以上，每段视频持续10秒左右。类别主要分为三大类：人与物互动，比如演奏乐器；人人互动，比如握手、拥抱；运动等。即person、person-person、person-object。</p>
<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><h2 id="数据集类别介绍"><a href="#数据集类别介绍" class="headerlink" title="数据集类别介绍"></a>数据集类别介绍</h2><p>下面这些介绍主要是参考于17年deepmind发的论文”The Kinetics Human Action Video Dataset”，当时还是400类的数据集。论文请见。</p>
<p>数据集主要关注人类行为，action类的列表包括：</p>
<p>单人行为，例如绘画，喝酒，大笑，抽拳;</p>
<p>人人行为，例如拥抱，亲吻，握手；</p>
<p>人物行为，例如打开礼物，修剪草坪，洗碗。</p>
<p>一些行动是比较细粒度的，需要时序推理来区分，例如，不同类型的游泳。其他动作类别需要强调区分对象，例如演奏不同类型的乐器。</p>
<p>这些类别并没有严格的层级划分，但是还是有些的，类似父类子类关系，例如音乐类（打鼓、长号、小提琴、……），个人卫生类（刷牙、剪指甲、洗手、……），跳舞类（芭蕾、macarena、tap、……），烹饪(切割、煎、脱皮、……）等。</p>
<p>数据集有400个类别，每个动作都有400-1150个视频片段，每段视频的时长都在10秒左右。目前的版本有306245视频，分为三个部分，训练时每个类为250-1000个视频，验证时每个类50个视频，测试时每个类100个视频。</p>
<p>每个类都包含了一种行动。但是，一个特定的剪辑可以包含<br>几种动作。例如，开车”时“发短信”;“弹奏尤克里里”时“跳草裙舞”;“跳舞”时“刷牙”。这种情况下，这个视频只会标记一个标签，并不会同时存在于两个类种。因此，取top-5的准确率更为合适。</p>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>一共验证了三种处理行为识别的主流模型，分别是LSTM、two stream和3dcnn。<br><img src="https://i.loli.net/2018/06/02/5b12089d68a3f.jpg" alt="">   </p>
<p>放上准确率，可以看出two-stream还是占据主导优势的。在kinetics数据集上，top-1是61.0，top-5是81.3。<br><img src="https://i.loli.net/2018/06/02/5b1208bda6f55.jpg" alt="">  </p>
<h2 id="分类结果分析"><a href="#分类结果分析" class="headerlink" title="分类结果分析"></a>分类结果分析</h2><p><img src="https://i.loli.net/2018/06/02/5b1208f97135e.jpg" alt="">  </p>
<p>上图是那些类分的比较好，那些类分的比较差。</p>
<p>还有一点，由于很多动作比较细粒度，类别之间容易造成混淆，论文中也给出了最容易混淆的几个类别，比如，跳远和三级跳远，吃汉堡和吃甜甜圈。swing跳舞和跳萨尔萨舞等都会混淆。</p>
<p>如下图：<br><img src="https://i.loli.net/2018/06/02/5b12091f8f6b5.jpg" alt="">  </p>
<p>当然，由于使用two stream模型，光流模型和RGB模型可能对不同的动作有不同的准确度，对于这些特定类别，可以在融合时对光流和RGB设定不同的权重。<br><img src="https://i.loli.net/2018/06/02/5b12094bd1965.jpg" alt="">  </p>
<h2 id="粒度划分"><a href="#粒度划分" class="headerlink" title="粒度划分"></a>粒度划分</h2><p>最后给出这些类别的一个粒度划分，可能有多个类别都会属于同一大类。即父类子类关系。</p>
<p>首先列出有哪些父类，然后再给出每个父类下的子类。</p>
<h3 id="父类："><a href="#父类：" class="headerlink" title="父类："></a>父类：</h3><p>共38大类，每个类后面的数字代表有几个子类。</p>
<p><img src="https://i.loli.net/2018/06/02/5b1209d4606a5.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b1209f3b13d7.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a1262d07.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a3884cf5.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a62ee102.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a828fe0e.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120ac63f9b1.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120af45341c.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b18ecaa3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b2b19ba4.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b497dbb3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b8f3a289.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120bb8b3fb7.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120bdac4ade.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c069dbda.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c250c117.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c3d062f0.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c61e5fa3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c7f966eb.jpg" alt="">  </p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman, The Kinetics Human Action Video Dataset, 2017</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_AVA（atomic visual actions）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_AVA（atomic visual actions）/" itemprop="url">AVA（atomic visual actions）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://research.google.com/ava/" target="_blank" rel="noopener">http://research.google.com/ava/</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>教机器理解视频中的人的行为是计算机视觉中的一个基本研究问题，谷歌blog发布了一个堪比“视频版”ImageNet的数据集-AVA（A Finely Labeled Video Dataset for Human Action Understanding ），旨在教机器理解人的活动。 该数据集以人类为中心进行标注，包含80类动作的 57600 个视频片段，有助于人类行为识别系统的研究。
　　  </p>
<p>教机器理解视频中的人的行为是计算机视觉中的一个基本研究问题，对个人视频搜索和发现、运动分析和手势界面等应用十分重要。尽管在过去的几年里，对图像进行分类和在图像中寻找目标对象方面取得了令人兴奋的突破，但识别人类的动作仍然是一个巨大的挑战。这是因为动作的定义比视频中的对象的定义要差，因此很难构造一个精细标记的动作视频数据集。许多基准数据集，例如 UCF101、activitynet 和DeepMind 的 Kinetics，都是采用图像分类的标记方案，在数据集中为每个视频或视频片段分配一个标签，而没有数据集能用于包含多个可能执行不同动作的人的复杂场景。</p>
<p>谷歌上周发布一个新的电影片段数据集，旨在教机器理解人的活动。这个数据集被称为 AVA（atomic visual action），这些视频对人类来说并不是很特别的东西——仅仅是 YouTube 上人们喝水、做饭等等的3秒钟视频片段。但每段视频都与一个文件捆绑在一起，这个文件勾勒了机器学习算法应该观察的人，描述他们的姿势，以及他们是否正在与另一个人或物进行互动。就像指着一只狗狗给一个小孩看，并教他说“狗！”，这个数据集是这类场景的数字版本。</p>
<h1 id="数据集特点"><a href="#数据集特点" class="headerlink" title="数据集特点"></a>数据集特点</h1><p>相比其他的动作数据集，AVA数据集有以下这些特点：</p>
<p><strong>以人为中心进行标注</strong>：每个动作标签都基于人物本身，而不是一段视频或者剪辑片段。因此，我们能够为不同动作中的各类人加上不同的标签，这一点非常常见。</p>
<p><strong>原子级视觉动作</strong>：我们对需要标注的动作进行了合理的时间限制（3秒钟），以确保动作符合人的生理机能，同时有明显的视觉特征。</p>
<p><strong>真实视频作为视觉材料</strong>：我们使用不同题材和国家的电影作为AVA的标注材料，进而确保数据库中包含各类型的人类行为。  </p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/wc7YNPm3YxVcUeMuMHYKbUFVF2ibGkUgw490HgjQfU5kN8NVYbPBNbJgZR9pJ1BN8Mc5iaPIdAicKLtwmsONEDvYg/0?wx_fmt=gif" alt="">  </p>
<p>视频来源中的3秒视觉片段标签，用方框标注出每个动作素材（为确保清晰，每个例子中只出现了一个框。）</p>
<p>为创建 AVA，我们首先从 YouTube 上收集了大量多样化的数据，主要集中在「电影」和「电视」类别，选择来自不同国家的专业演员。我们对每个视频抽取 15 分钟进行分析，并统一将 15 分钟视频分割成 300 个非重叠的 3 秒片段。采样遵循保持动作序列的时间顺序这一策略。</p>
<p>接下来，我们为每个 3 秒片段中间帧的人物手动标注边界框。对标注框中的每个人，标注者从预制的原子动作词汇表（80 个类别）中选择适当数量的标签来描述人物动作。这些动作可分为三组：姿势／移动动作、人-物互动和人-人互动。我们对执行动作的所有人进行了全部标注，因此 AVA 的标签频率遵循长尾分布，如下图所示。 </p>
<p><img src="http://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxVcUeMuMHYKbUFVF2ibGkUgwH3wDtoVY30WySELT8FopyRb1C7X31BoDib7OKBmKVDa2CDlx8HUtkQQ/0?wx_fmt=png" alt="">  </p>
<p>AVA 的原子动作标签分布。x 轴所示标签只是词汇表的一部分。</p>
<p>AVA 的独特设计使我们能够获取其他现有数据集中所没有的一些有趣数据。例如，给出大量至少带有两个标签的人物，我们可以判断动作标签的共现模式（co-occurrence pattern）。下图显示 AVA 中共现频率最高的动作对及其共现得分。我们确定的期望模式有：人们边唱歌边弹奏乐器、拥吻等。<br> <img src="http://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxVcUeMuMHYKbUFVF2ibGkUgwsojISBHw3F5O3pGhnDIDB6MNXe72falLbIWcbicgCdQhEIULVlSmZ1A/0?wx_fmt=png" alt=""><br>AVA 中共现频率最高的动作对。</p>
<p>为评估基于 AVA 数据集的人类动作识别系统的高效性，我们使用一个现有的基线深度学习模型在规模稍小一些的 JHMDB dataset 上取得了具备高竞争性的性能。由于存在可变焦距、背景杂乱、摄影和外观的不同情况，该模型在 JHMDB dataset 上的性能与在 AVA 上准确识别动作的性能（18.4% mAP）相比稍差。这表明，未来 AVA 可以作为开发和评估新的动作识别架构和算法的测试平台。  </p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik, AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions, 2017</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/卢梦依_语义关系分类数据集-semeval2007Task4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/卢梦依_语义关系分类数据集-semeval2007Task4/" itemprop="url">语义关系分类数据集-semeval2007Task4</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依<br>下载地址：<a href="https://github.com/davidsbatista/Annotated-Semantic-Relationships-Datasets/blob/master/datasets/SemEval2007-Task4.tar.gz" target="_blank" rel="noopener">https://github.com/davidsbatista/Annotated-Semantic-Relationships-Datasets/blob/master/datasets/SemEval2007-Task4.tar.gz</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>Task 4的主要任务是简单名词(名词或基本名词短语)之间的语义关系的分类，例如，蜜蜂，显示了产品生产者关系的一个实例。这种分类发生在书面英语文本中的一个句子的语境中。语义关系分类算法可以应用于信息检索、信息提取、文本摘要、问答等方面。对文本蕴涵(Tatu和Moldovan, 2005)的认识是在高端NLP应用中成功使用这种类型的深入分析的一个例子。</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p> 大小：小数据集，包含7个关系类型和总共1529个注释示例。</p>
<p> 示例：<br><img src="https://i.loli.net/2018/05/27/5b09c465cdde2.jpg" alt=""></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>1.T. Chklovskiand P. Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Proc.Conf.onEmpiricalMethodsin NaturalLanguageProcessing, EMNLP-04, pages 33–40, Barcelona, Spain.<br>2.R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19:479–496.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/刘唯_AI2科学问答数据集(多选)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/刘唯_AI2科学问答数据集(多选)/" itemprop="url">AI2科学问答数据集(多选)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘唯<br>下载地址：<a href="https://www.kaggle.com/allenai/ai2-science-questions" target="_blank" rel="noopener">https://www.kaggle.com/allenai/ai2-science-questions</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>Allen Institute for Artificial Intelligence (AI2)的Project Aristo专注于构建一个系统，该系统能够获取并存储大量的可计算形式的知识，然后将这些知识应用于不同年级水平的学生的标准化考试中的各种科学问题。我们邀请更广泛的人工智能研究社区，通过提供学生科学评估问题的数据集，来与我们共同应对这一重大挑战。<br>这些都是英语语言问题，它跨越了文件中所显示的几个年级水平。每个问题都是对应4个选择回答。其中一些问题包括一个图表，作为问题文本的一部分，作为回答选项，或者两者兼而有之。图在文本中表示，文件名对应于对应文件夹中的图文件本身。这些问题被预先划分为培训、开发和测试集。<br>数据集包括以下字段:<br>questionID: a unique identifier for the question<br>originalQuestionID: the question number on the test<br>totalPossiblePoints: how many points the question is worth<br>AnswerKey: the correct answer option<br>isMultipleChoiceQuestion: 1 = multiple choice, 0 = other<br>includesDiagram: 1 = includes diagram, 0 = other<br>examName: the source of the exam<br>schoolGrade: grade level<br>year: year the source exam was published<br>question: the question itself<br>subject: Science<br>category: Test, Train, or Dev (data comes pre-split into these categories)</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p>大小：56MB</p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>1.Clark, Peter. “Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!” AAAI (2015).</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/刘唯_完形填空(多选阅读理解)数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/刘唯_完形填空(多选阅读理解)数据集/" itemprop="url">完形填空(多选阅读理解)数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘维<br>下载地址：<a href="https://tticnlp.github.io/who_did_what/index.html" target="_blank" rel="noopener">https://tticnlp.github.io/who_did_what/index.html</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>我们已经构建了一个新的“Who-did-What”数据集，该数据集包含了来自LDC英语Gigaword newswire语料库构建的超过20万填充物(cloze)的多重选择阅读理解问题。WDW数据集具有多种新特性。首先，与CNN和每日邮件数据(Hermann et al.， 2015)相比，我们避免使用文章摘要来回答问题。相反，每一个问题都是由两篇独立的文章组成的——一篇文章作为一篇文章，另一篇文章是关于同一事件的一篇文章。第二，我们避免匿名化——每个选择都是一个人的名字。第三，这些问题被过滤掉，去掉了一个简单的基线可以轻易解决的分数，而剩下的84%由人类来解决。我们报告了标准系统的性能基准，并提出WDW数据集作为社区的一项挑战任务。</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p> 大小：包含了37322个50个动物的图像。<br> 1.CUHK student data set 含188张faces<br> 2.AR data set (123 faces)<br> 3.XM2VTS data set (295 faces)</p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Y. Xian, C. H. Lampert, B. Schiele, Z. Akata. “Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly” arXiv:1707.00600</p>
<p>[2] C. H. Lampert, H. Nickisch, and S. Harmeling. “Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer”. In CVPR, 2009<br>[3] C. H. Lampert, H. Nickisch, and S. Harmeling. “Attribute-Based Classification for Zero-Shot Visual Object Categorization”. IEEE T-PAMI, 2013</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/刘唯_动物属性标记数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/刘唯_动物属性标记数据集/" itemprop="url">动物属性标记数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘维<br>下载地址：<a href="http://cvml.ist.ac.at/AwA2/" target="_blank" rel="noopener">http://cvml.ist.ac.at/AwA2/</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>该数据集提供了一个平台，用于基准的转移学习算法，特别是属性基分类和零射学习。它可以充当原始动物的替代，使用属性(AwA)数据集，因为它具有相同的类结构和几乎相同的特征。它包含了37322个50个动物的图像，每个图像都有预先提取的特征表示。这些类与Osherson的经典类/属性矩阵一致，从而为每个类提供85个数字属性值。使用共享属性，可以在不同的类之间传递信息。这些图像数据是在2016年从Flickr等公共资源中收集的。</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p> 大小：包含了37322个50个动物的图像。<br> 1.CUHK student data set 含188张faces<br> 2.AR data set (123 faces)<br> 3.XM2VTS data set (295 faces)</p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Y. Xian, C. H. Lampert, B. Schiele, Z. Akata. “Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly” arXiv:1707.00600</p>
<p>[2] C. H. Lampert, H. Nickisch, and S. Harmeling. “Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer”. In CVPR, 2009<br>[3] C. H. Lampert, H. Nickisch, and S. Harmeling. “Attribute-Based Classification for Zero-Shot Visual Object Categorization”. IEEE T-PAMI, 2013<br>[4]X. Tang, and X. Wang, “Face Photo Recognition Using Sketch,” in Proceedings of IEEE International Conference on Image Processing (ICIP), Vol. 1, pp. 257-260, Rochester, New York, Sept. 2002.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/刘唯_人脸素描数据集 /">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/刘唯_人脸素描数据集 /" itemprop="url">人脸素描数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘维<br>下载地址：<a href="http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>中大脸部素描数据库(CUFS)是面向人脸素描合成和人脸素描识别的研究。它包括来自香港中文大学(中大)学生数据库的188张脸，来自AR数据库的123张脸，以及295张来自XM2VTS数据库的面孔。总共有606张脸。对于每张脸，都有一幅画是由一位艺术家绘制的。</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p> 大小：包含3个文件数据文件。<br> 1.CUHK student data set 含188张faces<br> 2.AR data set (123 faces)<br> 3.XM2VTS data set (295 faces)</p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><ol>
<li><p>X. Wang and X. Tang, “Face Photo-Sketch Synthesis and Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), Vol. 31, 2009.</p>
</li>
<li><p>Qingshan Liu, Xiaoou Tang, Hongliang Jin, Hanqing Lu, and Songde Ma,  A Nonlinear Approach For Face Sketch Synthesis and Recognition,  Int’l Conf. on Computer Vision and Pattern Recognition (CVPR), 2005.</p>
</li>
<li><p>X. Tang, and X. Wang, “Face Sketch Recognition,” IEEE Transactions on Circuits and Systems for Video Technology (CSVT), Special Issue on Image- and Video- Based Biometrics, Vol. 14, No. 1, pp. 50-57, January, 2004.</p>
</li>
<li><p>X. Tang, and X. Wang, “Face Sketch Synthesis and Recognition,” in Proceedings of IEEE International Conference on Computer Vision (ICCV), 2003.</p>
</li>
<li><p>X. Tang, and X. Wang, “Face Photo Recognition Using Sketch,” in Proceedings of IEEE International Conference on Image Processing (ICIP), Vol. 1, pp. 257-260, Rochester, New York, Sept. 2002.</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/25/朱述承_中研院中古汉语标记语料库/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/25/朱述承_中研院中古汉语标记语料库/" itemprop="url">中研院中古汉语标记语料库</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-25T15:37:46+05:00">
                2018-05-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：朱述承<br>访问地址：<a href="http://lingcorpus.iis.sinica.edu.tw/middle/" target="_blank" rel="noopener">http://lingcorpus.iis.sinica.edu.tw/middle/</a></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>“中央研究院中古汉语标记语料库”是“中央研究院古汉语语料库”(Academia Sinica Ancient Chinese Corpus)的一个次语料库。“中央研究院古汉语语料库”是应汉语史研究需求而建构的语料库。这个语料库又可依是否经过断词及加标词类而分成两类，即未加标的素语料库以及有标注的标记语料库。目前素语料库所收集的语料已含盖上古汉语（先秦至西汉）、中古汉语（东汉魏晋南北朝）、近代汉语（唐五代以后）大部分的重要语料，并已陆续开放使用；在标记语料库方面，上古汉语及近代汉语都已有部分语料完成标注的工作，并视结果逐步提供线上检索。“中央研究院古汉语语料库”的建构始于一九九０年，创始者为黄居仁(语言所研究员)、谭朴森(英国伦敦大学亚非学院教授)、陈克健(资讯所研究员)、魏培泉(语言所研究员)等，最初的经费来源为蒋经国基金会及中央研究院历史语言研究所，目标是收集上古汉语的素语料。素语料库的构建自此未曾停歇，语料也由上古汉语扩充到中古汉语和近代汉语。</p>
<h1 id="使用限制"><a href="#使用限制" class="headerlink" title="使用限制"></a>使用限制</h1><p>院内检索限制两万行资料，院外检索限两千行资料。</p>
<h1 id="功能简介"><a href="#功能简介" class="headerlink" title="功能简介"></a>功能简介</h1><p>透过这个语料库的介面可以进行下列几项工作：<br>一、检索：首先进入“自订语料库”的画面，设定文献的搜寻范围，接著进入“内容检索”与“进阶处理”的画面，在自订语料库范围内针对词项、词头、词尾、词类、词类特征、重叠词型态……等进行检索以及进阶检索；<br>二、显示：有两种资料，“在关键词检索典”画面上，将检索到的资料依句显示在屏幕上，“文本”的画面出现该关键词所出现的该章回段落；<br>三、过滤：依照使用者设定的条件筛选语料；<br>四、词类累计：统计每个词类出现的数量；<br>五、统计共现率（collocation）；<br>六、排序：针对使用者设定的条件将语料依序排列。 </p>
<h1 id="文献内容"><a href="#文献内容" class="headerlink" title="文献内容"></a>文献内容</h1><p>抱朴子内篇  世说新语  新校搜神记  洛阳伽蓝记  颜氏家训<br>道行般若经  佛说兜沙经  阿门佛国经  佛说遗日摩尼宝经  佛说般舟三昧经<br>般舟三昧经  文殊师利问菩萨署经  法镜经  阿含口解十二因缘经  中本起经<br>修行本起经  梵摩渝经  佛说义足经  大明度经  佛说菩萨本业经<br>了本生死经  佛说四愿经  六度集经  生经  佛说普曜经<br>光讚经  大楼炭经  阿育王传  出曜经  大庄严论经<br>妙法莲华经  悲华经  百喻经  佛本行集经  佛说伅真陀罗所问如来三昧经<br>佛说阿闍世王经  齐民要术 </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">CNLR</p>
              <p class="site-description motion-element" itemprop="description">语料库、数据集及工具资源和教程</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">164</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CNLR</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
