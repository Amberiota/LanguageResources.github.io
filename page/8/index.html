<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="语料库、数据集及工具资源和教程">
<meta property="og:type" content="website">
<meta property="og:title" content="世界语言资源平台">
<meta property="og:url" content="http://yoursite.com/page/8/index.html">
<meta property="og:site_name" content="世界语言资源平台">
<meta property="og:description" content="语料库、数据集及工具资源和教程">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="世界语言资源平台">
<meta name="twitter:description" content="语料库、数据集及工具资源和教程">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/8/"/>





  <title>世界语言资源平台</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">世界语言资源平台</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_Kinetics-600 dataset/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_Kinetics-600 dataset/" itemprop="url">Kinetics-600 dataset</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/" target="_blank" rel="noopener">https://deepmind.com/research/open-source/open-source-datasets/kinetics/</a>  </p>
<h1 id="背景与来源"><a href="#背景与来源" class="headerlink" title="背景与来源"></a>背景与来源</h1><p>目前activitynet第三届比赛已经开始了，这项比赛始于2016CVPR，是与ImageNet齐名的在视频理解方面最重要的比赛。</p>
<p>在这个比赛下的Task A – Trimmed Action Recognition比赛是一个视频分类比赛，数据集就是kinetics-600数据集。数据集有Google的deepmind团队提供，2017年是第一届比赛，当时有400个类别，20多万数据，今年又对数据集进行了扩增，现在有600个类别，共50万左右的视频。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>视频来源于YouTube，一共有600个类别，每个类别至少600个视频以上，每段视频持续10秒左右。类别主要分为三大类：人与物互动，比如演奏乐器；人人互动，比如握手、拥抱；运动等。即person、person-person、person-object。</p>
<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><h2 id="数据集类别介绍"><a href="#数据集类别介绍" class="headerlink" title="数据集类别介绍"></a>数据集类别介绍</h2><p>下面这些介绍主要是参考于17年deepmind发的论文”The Kinetics Human Action Video Dataset”，当时还是400类的数据集。论文请见。</p>
<p>数据集主要关注人类行为，action类的列表包括：</p>
<p>单人行为，例如绘画，喝酒，大笑，抽拳;</p>
<p>人人行为，例如拥抱，亲吻，握手；</p>
<p>人物行为，例如打开礼物，修剪草坪，洗碗。</p>
<p>一些行动是比较细粒度的，需要时序推理来区分，例如，不同类型的游泳。其他动作类别需要强调区分对象，例如演奏不同类型的乐器。</p>
<p>这些类别并没有严格的层级划分，但是还是有些的，类似父类子类关系，例如音乐类（打鼓、长号、小提琴、……），个人卫生类（刷牙、剪指甲、洗手、……），跳舞类（芭蕾、macarena、tap、……），烹饪(切割、煎、脱皮、……）等。</p>
<p>数据集有400个类别，每个动作都有400-1150个视频片段，每段视频的时长都在10秒左右。目前的版本有306245视频，分为三个部分，训练时每个类为250-1000个视频，验证时每个类50个视频，测试时每个类100个视频。</p>
<p>每个类都包含了一种行动。但是，一个特定的剪辑可以包含<br>几种动作。例如，开车”时“发短信”;“弹奏尤克里里”时“跳草裙舞”;“跳舞”时“刷牙”。这种情况下，这个视频只会标记一个标签，并不会同时存在于两个类种。因此，取top-5的准确率更为合适。</p>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>一共验证了三种处理行为识别的主流模型，分别是LSTM、two stream和3dcnn。<br><img src="https://i.loli.net/2018/06/02/5b12089d68a3f.jpg" alt="">   </p>
<p>放上准确率，可以看出two-stream还是占据主导优势的。在kinetics数据集上，top-1是61.0，top-5是81.3。<br><img src="https://i.loli.net/2018/06/02/5b1208bda6f55.jpg" alt="">  </p>
<h2 id="分类结果分析"><a href="#分类结果分析" class="headerlink" title="分类结果分析"></a>分类结果分析</h2><p><img src="https://i.loli.net/2018/06/02/5b1208f97135e.jpg" alt="">  </p>
<p>上图是那些类分的比较好，那些类分的比较差。</p>
<p>还有一点，由于很多动作比较细粒度，类别之间容易造成混淆，论文中也给出了最容易混淆的几个类别，比如，跳远和三级跳远，吃汉堡和吃甜甜圈。swing跳舞和跳萨尔萨舞等都会混淆。</p>
<p>如下图：<br><img src="https://i.loli.net/2018/06/02/5b12091f8f6b5.jpg" alt="">  </p>
<p>当然，由于使用two stream模型，光流模型和RGB模型可能对不同的动作有不同的准确度，对于这些特定类别，可以在融合时对光流和RGB设定不同的权重。<br><img src="https://i.loli.net/2018/06/02/5b12094bd1965.jpg" alt="">  </p>
<h2 id="粒度划分"><a href="#粒度划分" class="headerlink" title="粒度划分"></a>粒度划分</h2><p>最后给出这些类别的一个粒度划分，可能有多个类别都会属于同一大类。即父类子类关系。</p>
<p>首先列出有哪些父类，然后再给出每个父类下的子类。</p>
<h3 id="父类："><a href="#父类：" class="headerlink" title="父类："></a>父类：</h3><p>共38大类，每个类后面的数字代表有几个子类。</p>
<p><img src="https://i.loli.net/2018/06/02/5b1209d4606a5.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b1209f3b13d7.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a1262d07.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a3884cf5.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a62ee102.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a828fe0e.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120ac63f9b1.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120af45341c.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b18ecaa3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b2b19ba4.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b497dbb3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b8f3a289.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120bb8b3fb7.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120bdac4ade.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c069dbda.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c250c117.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c3d062f0.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c61e5fa3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c7f966eb.jpg" alt="">  </p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman, The Kinetics Human Action Video Dataset, 2017</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_Moments-in-Time/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_Moments-in-Time/" itemprop="url">Moments in Time</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://link.zhihu.com/?target=http%3A//moments.csail.mit.edu/" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//moments.csail.mit.edu/</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>在过去一年中，视频理解相关的领域涌现了大量的新模型、新方法，与之相伴的，今年也出现了多个新的大规模的视频理解数据集。近期，MIT-IBM Watson AI Lab 就推出了一个全新的百万规模视频理解数据集Moments-in-Time虽然没有之前的YouTube-8M数据集大，但应该是目前多样性，差异性最高的数据集了。该数据集的任务仍然为视频分类任务，不过其更专注于对“动作”的分类，此处的动作为广义的动作或动态，其执行者不一定是人，也可以是物体或者动物，这点应该是该数据集与现有数据集最大的区分。本文中简单的统称为“动作”。</p>
<h1 id="数据集概览"><a href="#数据集概览" class="headerlink" title="数据集概览"></a>数据集概览</h1><p>这部分主要对数据集的基本情况和特性进行介绍，大概可以总结为以下几点：</p>
<ul>
<li>共有100,0000个视频，每个视频的长度相同，均为3s</li>
<li>每个视频有一个动作标签（后续版本可能拓展为多标签），此处的动作仅为动词，比如“opening”就为一个标签（与之不同，其他数据集经常会采用动名词组的形式如”opening the door”）</li>
<li>动作主体可以是人，动物，物体乃至自然现象。</li>
<li>数据集的类内差异和类间差异均很大。</li>
<li>存在部分或完全依赖于声音信息的动作，如clapping（拍手）<br>由上述描述可以看出，由于超大的数据量以及多样性，这个数据集是相当难的，下图则为该数据集的一个例子。可以看出，一个动作类别可以由多种动作主体完成，从而从视觉上看的差异性相当的大，动作的概念可以说是相当抽象了。<br><img src="https://i.loli.net/2018/06/02/5b1212dc40bc6.jpg" alt=""></li>
</ul>
<p>下面我对作者构建这个数据集的方式进行介绍，这部分内容也有助于对该数据集的理解。</p>
<h1 id="数据集的构建"><a href="#数据集的构建" class="headerlink" title="数据集的构建"></a>数据集的构建</h1><h2 id="建立动作的字典"><a href="#建立动作的字典" class="headerlink" title="建立动作的字典"></a>建立动作的字典</h2><p>该数据集采用的是先确定动作标签，再根据动作标签构建视频集合的方式。构建动作标签集合，在该数据集中即构建一个合适的动作字典。主要通过以下几个步骤实现：</p>
<ul>
<li>参考[2]中的内容，选取4500个美式英语中最常用的动词</li>
<li>按照词义对这4500个词进行聚类，一个动词可以属于多个聚类</li>
<li>迭代的从最常见的聚类中选取最常见的动词加入目标字典</li>
<li>最终从4500个初始动词中选取339个最常见的动词作为字典  </li>
</ul>
<p><img src="https://i.loli.net/2018/06/02/5b1212f219cc3.jpg" alt=""></p>
<h2 id="数据收集与标注"><a href="#数据收集与标注" class="headerlink" title="数据收集与标注"></a>数据收集与标注</h2><p>在确定好动词字典后，作者对每个动词，在多个视频网站上进行视频的爬取。这里的视频网站比较多，包含YouTube，Flicker，Vine等十几个网站，比起只用YouTube的ActivityNet，Kinectic等数据集在来源的丰富性上要高不少。</p>
<p>在爬完数据后，每个视频都是以 视频-动词 对的形式呈现，标注工作的主要目的就是确定视频是否可以用动词描述，所以是一个二分类的标注任务（此处作者的解释是，多分类的标注对于标注者难度太高，也容易错，故采用二分类的标注方式）。标注工作在近来大量数据集都采用的Amazon Mechanical Turk实现。</p>
<p>对于每个标注者，都会被分配64个待标注的动词-视频对以及10个已知真值的动词-视频对。在10个已知真值的动词-视频对中，只有标对9个及以上，该标注者的标注结果才会被认为是有效的。剩下的所有动词-视频对，都会被交由2个标注者，只有俩人的标注结果一致，该结果才会被采用。所以从标注角度来看，这个数据集的标签质量应该还是不错的。标注界面的样式如下图所示，可以看出还是相当简洁明了的。<br><img src="https://i.loli.net/2018/06/02/5b121a603d679.jpg" alt=""></p>
<h1 id="数据集的数据分布"><a href="#数据集的数据分布" class="headerlink" title="数据集的数据分布"></a>数据集的数据分布</h1><p>接下来我主要对该数据集的数据分布进行介绍，由于该数据集目前还没有正式放出，所以所有数据和图表均来自论文。</p>
<p>首先是数据集的类别分布：</p>
<ul>
<li>对于339个动作类别，共有超过100000个标注视频</li>
<li>每个类别至少有1000个视频，每个类别视频数量的平均值是1757，中值是2775</li>
</ul>
<p>类别与类别视频数量的关系图如下图所示。  </p>
<p><img src="https://i.loli.net/2018/06/02/5b12127aa3e93.jpg" alt=""></p>
<p>接下来，作者介绍了数据集中动作主体的分布情况，如前所述动作主题可能是人，动物或一般物体。作者统计了不同类别视频中各类动作主题所占比例的分布，如下图所示。左侧的极端是“typing“，主体全部是人类，右边的极端是”overflowing”,动作主题基本不是人类。<br><img src="https://i.loli.net/2018/06/02/5b121295f1aba.jpg" alt=""></p>
<p>最后，作者分析了数据集各个类别中依赖于声音的视频所在的比例。此处，依赖于声音的视频是指该视频无法从图像上判断出其包含的动作，而必须要听声音。从下图可以看出，有相当比例的视频是依赖于声音的，这点要增加了该数据集的挑战性。</p>
<p><img src="https://i.loli.net/2018/06/02/5b1212a55102b.jpg" alt=""></p>
<h1 id="场景、物体与动作之前的相关性探索"><a href="#场景、物体与动作之前的相关性探索" class="headerlink" title="场景、物体与动作之前的相关性探索"></a>场景、物体与动作之前的相关性探索</h1><p>最后，作者通过一组简单的实验探索了各个数据集中 物体-场景-动作 之间的相关性。此处分析的视频数据集除了Moments in Time外， 还包括UCF-101, ActivityNet 1.3 以及Kinetics数据集。</p>
<p>这里的实验设置还蛮有趣的。作者分别采用了一个在ImageNet上训练的Resnet50用于物体分类，一个在Places数据集上训练的Resnet50用作场景分类。对于每个视频，均匀抽取3帧并利用两个网络进行检测并平均结果，可以得到一个物体label以及一个场景label。对于物体或场景label，作者通过贝叶斯公式来推断对应的动作类别，其中先验概率在数据集的训练集上计算获得。</p>
<p>实验结果如下表所示，可以得到以下几点结论：</p>
<ul>
<li>动作与场景以及物体均是相关的。</li>
<li>Moments in Time数据集中，动作与物体以及场景的相关性显著弱于其他几个数据集，这表明该数据集有更高的挑战性以及更大的难度。<br><img src="https://i.loli.net/2018/06/02/5b1212be03883.jpg" alt=""></li>
</ul>
<h1 id="个人讨论"><a href="#个人讨论" class="headerlink" title="个人讨论"></a>个人讨论</h1><p>Moments-in-Time数据集我觉得还是相当有趣以及有挑战性的，估计很快就会有不少人跟进来做这个数据集（显而易见需要比较大的计算资源…）。下面是我对于该数据集的一些讨论内容，包括优点以及一些个人存在疑惑的地方。</p>
<p><strong>优点：</strong></p>
<ul>
<li>数据集的大小和丰富程度很高，足以训练较复杂的视频分类模型。</li>
<li>视频的长度统一为3s，这样的设计方便实验时进行处理，也使得数据集的尺寸不至于过大。</li>
<li>数据标注的策略应该还是比较靠谱的，应该不太会有错误标注。</li>
</ul>
<p>以上是几点明显的优点，但对于作者强调的几个数据集优点，我则存在一些疑惑：</p>
<ul>
<li>仅用动词定义动作：这个应该是这个数据集和其他数据集相比最大的一个差异点。作者认为通过该数据集能够学习一个泛化能力很强的动作概念，但在我看来这样的定义有些太过宽泛了。动词的含义常常依赖于其主语和谓语，单独的动词即便对于人类而言也常常是含义模糊的。此处可以参考今年ICCV上的[3]一文，我此前也写过一篇笔记：<a href="https://zhuanlan.zhihu.com/p/29227174" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29227174</a> 介绍这篇文章。这篇文章中一个重要的观点是，动作应该用动词-名词组合来定义，从而明确其含义。不过该数据集也是故意在此处模糊化从而增加类内差异，现在也不能够知道是否是一个好的设计了。</li>
<li>动作的主体不一定是人：这点也是数据集作者有意设计，从而增加难度以及多样性。我也持有同样的对于定义不清晰的疑惑，比如人开门（“opening”）和风吹开了一扇窗户（”opening“）放在同一个类别中总感觉不太合理。此外，此处还有一个问题，尽管温中给出了动作主体的分析，但通过询问作者，第一版的数据集不会提供动作主体的label，而仅包含一个动作label。</li>
<li>依赖声音的动作：这点我觉得倒是蛮好的，可以促进多模态方法的发展。但是同以上一点，该数据集在训练集中并没有告知这个视频中的动作是否是依赖与声音的。如果有相关的标签，我觉得会更有助于视频的理解吧。作者可能会在后续版本加上。<br>总体而言，这个新数据集还是很有趣且充满挑战的，与此前的多个主要关注人类动作的数据集在设定上有较大的差异。针对这个数据集，模型方面应该更注重于对动作概念的理解以及对较大的类内差异性的处理。期待之后针对该数据集的算法了。</li>
</ul>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Monfort M, Zhou B, Bargal S A, et al. Moments in Time Dataset: one million videos for<br>event understanding[J].</p>
<p>[2] Salamon J, Jacoby C, Bello J P. A dataset and taxonomy for urban sound research[C]//Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014: 1041-1044.</p>
<p>[3] Sigurdsson G A, Russakovsky O, Gupta A. What Actions are Needed for Understanding<br>Human Actions in Videos?[J]. arXiv preprint arXiv:1708.02696, 2017.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_UCF101 - Action Recognition Data Set/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_UCF101 - Action Recognition Data Set/" itemprop="url">UCF101 - Action Recognition Data Set</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://crcv.ucf.edu/data/UCF101.php" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101.php</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>UCF101是一个动作识别数据集，包含现实的动作视频，从YouTube上收集，有101个动作类别。该数据集是UCF50数据集的扩展，该数据集有50个动作类别。<br>从101个动作类的13320个视频中，UCF101给出了最大的多样性，并且在摄像机运动、物体外观和姿态、物体尺度、视点、杂乱背景、光照条件等方面存在较大的差异，这是迄今为止最具挑战性的数据。<br>由于大多数可用的动作识别数据集都不现实，而且是由参与者进行的，UCF101旨在通过学习和探索新的现实行动类别来鼓励进一步研究行动识别。<br>101个动作类的视频被分成25组，每个组可以包含4-7个动作视频。同一组的视频可能有一些共同的特点，比如背景相似、观点相似等。  </p>
<p><strong>动作类别可以分为五类： </strong> </p>
<ul>
<li>Human-Object Interaction  </li>
<li>Body-Motion Only   </li>
<li>Human-Human Interaction   </li>
<li>Playing Musical Instruments   </li>
<li>Sports.   </li>
</ul>
<p>UCF101数据集的操作类别为:应用眼妆、唇膏、射箭、婴儿爬行,平衡木,乐队游行,棒球,篮球,篮球扣篮,卧推,骑自行车,台球,吹干头发,吹蜡烛,体重下蹲,保龄球,拳击出气筒,拳击袋速度,蛙泳,刷牙,挺举,悬崖跳水,板球保龄球,板球,削减在厨房,潜水,打鼓,击剑、曲棍球点球,地板体操,飞盘,爬泳,高尔夫挥杆,发型、链球、锤击,倒立俯卧撑,倒立行走,头部按摩,跳高,赛马,骑马、呼啦圈、冰上舞蹈,掷标枪,杂耍球,跳绳,跳杰克,皮划艇,针织,跳远,弓步,阅兵,搅拌面糊、拖地板,修女轻叩,双杠,披萨扔,弹吉他,弹钢琴,打手鼓,演奏小提琴,演奏大提琴,玩来说,玩时代,演奏长笛,玩锡塔尔琴,撑杆跳,鞍马、拉Ups、穿孔、俯卧撑、漂流、攀岩室内,绳索攀爬、划船、萨尔萨舞旋转,剃胡子,推铅球,滑板,滑雪,Skijet,跳伞,足球杂耍,足球点球,还是戒指,相扑,冲浪,秋千,乒乓球拍、太极、网球挥拍,扔铁饼,蹦床跳,打字,高低杠,排球飙升,与狗一起散步,“推墙”,写作,溜溜球。  </p>
<p>下载UCF101数据集：<a href="http://crcv.ucf.edu/data/UCF101/UCF101.rar" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101.rar</a>  </p>
<p>UCF101数据集的动作识别（ Action Recognition）的训练/测试集下载地址:<a href="http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip</a>  </p>
<p>UCF101数据集的动作检测（ Action Detection）的训练/测试集下载地址:<a href="http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-DetectionTask.zip" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-DetectionTask.zip</a>  </p>
<p>UCF101数据集的STIP特性可以在这里下载:<a href="http://crcv.ucf.edu/data/UCF101/UCF101_STIP_Part1.rar" target="_blank" rel="noopener">Part1</a>,<a href="http://crcv.ucf.edu/data/UCF101/UCF101_STIP_Part2.rar" target="_blank" rel="noopener">Part2</a>  </p>
<p><img src="https://i.loli.net/2018/06/02/5b121da3d8ad6.jpg" alt="">  </p>
<h1 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h1><p><img src="http://crcv.ucf.edu/data/UCF101/Clip%20Duration%201.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Clip%20Duration%202.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Number%20of%20Videos%201.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Number%20of%20Videos%202.jpg" alt="">  </p>
<p>注意：将属于同一组的视频保持在训练和测试中非常重要。由于一组中的视频是从单个长视频中获得的，因此在训练和测试套件中共享来自同一组的视频会获得较高的性能。 </p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Khurram Soomro, Amir Roshan Zamir and Mubarak Shah, UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild, CRCV-TR-12-01, November, 2012.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_CoPhIR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_CoPhIR/" itemprop="url">CoPhIR 数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓<br>地址：<a href="http://cophir.isti.cnr.it/whatis.html" target="_blank" rel="noopener">http://cophir.isti.cnr.it/whatis.html</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>雅虎发布的超大Flickr数据集，包含1亿多张图片。<br>CoPhIR（Content-based Photo Image Retrieval，基于内容的照片图像检索）测试集合的开发旨在对SAPIR项目基础设施（SAPIR：使用对等IR中的音频视频内容进行搜索）的可扩展性进行重要测试以进行相似性搜索。 CoPhIR是NMIS实验室与意大利比萨ISTI-CNR的HPC实验室共同努力的成果。 我们通过DILIGENT项目使用EGEE European GRID从Flickr存档提取元数据。 对于每个图像，已经提取了标准的MPEG-7图像特征。试验台的每个入口都包含：  </p>
<ul>
<li>链接Flickr网站的相应条目   </li>
<li>照片图像缩略图  </li>
<li>一个XML结构，在相应的Flickr条目中包含Flickr用户信息：标题，位置，GPS，标签，注释等。   </li>
<li><p>具有5个提取的标准MPEG-7图像特征的XML结构：  </p>
<ul>
<li>可伸缩的色彩  </li>
<li>色彩结构 </li>
<li>色彩布局  </li>
<li>边缘直方图</li>
<li>均匀纹理  </li>
</ul>
</li>
</ul>
<p>迄今收集的数据代表世界上最大的多媒体元数据收集，可用于可扩展相似性搜索技术的研究。 CoPhIR包含1.06亿个处理过的图像。   </p>
<p>CoPhIR现在可供研究人员尝试比较不同的索引技术进行相似性搜索，其中可扩展性是关键问题。  </p>
<p>我们使用Flickr图片内容符合Creative Commons许可。 CoPhIR测试集合符合基于WIPO（世界知识产权组织）版权条约和表演和录音制品条约以及意大利现行法律68/2003的欧洲第29/2001号建议书。<br>为了访问CoPhIR发行版，有兴趣在其上进行实验的组织（大学，研究实验室等）将必须签署随附的CoPhIR访问协议和CoPhIR访问注册表，将原始签名文件通过邮件发送给我们。请按照“如何获得CoPhIR测试集合”一节中的说明进行操作。然后，您将收到登录和密码以下载所需的文件。  </p>
<h1 id="使用–获得CoPhIR测试集"><a href="#使用–获得CoPhIR测试集" class="headerlink" title="使用–获得CoPhIR测试集"></a>使用–获得CoPhIR测试集</h1><ul>
<li>发送电子邮件到 cophiristi.cnr.it (subject: new access to Cophir)，包含有必要信息的请求(见<a href="http://cophir.isti.cnr.it/RequestTemplate.txt" target="_blank" rel="noopener">请求模板</a>)。  </li>
<li>打印CoPhIR Access Agreement和CoPhIR Access Registration Form (doc, pdf)，填写所需信息，然后由授权人签署正本文件。  </li>
<li>将两份文件邮寄至</li>
</ul>
<blockquote>
<p>Dr. Fausto Rabitti<br>NMIS Lab.<br>ISTI-CNR, Pisa Research Area<br>Via Moruzzi, 1<br>56124 Pisa (Italy).  </p>
</blockquote>
<ul>
<li><p>我们将发送到您的电子邮件地址，在访问注册表中显示，一封包含登录名和密码的电子邮件将用于访问CoPhIR测试集合。  </p>
</li>
<li><p>要下载CoPhIR测试集合的文件，请在CoPhIR网站上输入<a href="http://cophir.isti.cnr.it/download.html" target="_blank" rel="noopener">下载</a>部分并使用您的登录名和密码。  </p>
</li>
</ul>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] F Rabitti, R Perego,F Falchi,C Lucchese, P Bolettieri, CoPhIR (Content-based Photo Image Retrieval) Test-Collection, 2008<br>[2] M Batko,P Kohoutkova,D Novak, CoPhIR Image Collection under the Microscope, 2009  </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/刘唯_AI2科学问答数据集(多选)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/刘唯_AI2科学问答数据集(多选)/" itemprop="url">AI2科学问答数据集(多选)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘唯<br>下载地址：<a href="https://www.kaggle.com/allenai/ai2-science-questions" target="_blank" rel="noopener">https://www.kaggle.com/allenai/ai2-science-questions</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>Allen Institute for Artificial Intelligence (AI2)的Project Aristo专注于构建一个系统，该系统能够获取并存储大量的可计算形式的知识，然后将这些知识应用于不同年级水平的学生的标准化考试中的各种科学问题。我们邀请更广泛的人工智能研究社区，通过提供学生科学评估问题的数据集，来与我们共同应对这一重大挑战。<br>这些都是英语语言问题，它跨越了文件中所显示的几个年级水平。每个问题都是对应4个选择回答。其中一些问题包括一个图表，作为问题文本的一部分，作为回答选项，或者两者兼而有之。图在文本中表示，文件名对应于对应文件夹中的图文件本身。这些问题被预先划分为培训、开发和测试集。<br>数据集包括以下字段:<br>questionID: a unique identifier for the question<br>originalQuestionID: the question number on the test<br>totalPossiblePoints: how many points the question is worth<br>AnswerKey: the correct answer option<br>isMultipleChoiceQuestion: 1 = multiple choice, 0 = other<br>includesDiagram: 1 = includes diagram, 0 = other<br>examName: the source of the exam<br>schoolGrade: grade level<br>year: year the source exam was published<br>question: the question itself<br>subject: Science<br>category: Test, Train, or Dev (data comes pre-split into these categories)</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p>大小：56MB</p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>1.Clark, Peter. “Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!” AAAI (2015).</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/刘唯_完形填空(多选阅读理解)数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/刘唯_完形填空(多选阅读理解)数据集/" itemprop="url">完形填空(多选阅读理解)数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘维<br>下载地址：<a href="https://tticnlp.github.io/who_did_what/index.html" target="_blank" rel="noopener">https://tticnlp.github.io/who_did_what/index.html</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>我们已经构建了一个新的“Who-did-What”数据集，该数据集包含了来自LDC英语Gigaword newswire语料库构建的超过20万填充物(cloze)的多重选择阅读理解问题。WDW数据集具有多种新特性。首先，与CNN和每日邮件数据(Hermann et al.， 2015)相比，我们避免使用文章摘要来回答问题。相反，每一个问题都是由两篇独立的文章组成的——一篇文章作为一篇文章，另一篇文章是关于同一事件的一篇文章。第二，我们避免匿名化——每个选择都是一个人的名字。第三，这些问题被过滤掉，去掉了一个简单的基线可以轻易解决的分数，而剩下的84%由人类来解决。我们报告了标准系统的性能基准，并提出WDW数据集作为社区的一项挑战任务。</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p> 大小：包含了37322个50个动物的图像。<br> 1.CUHK student data set 含188张faces<br> 2.AR data set (123 faces)<br> 3.XM2VTS data set (295 faces)</p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Y. Xian, C. H. Lampert, B. Schiele, Z. Akata. “Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly” arXiv:1707.00600</p>
<p>[2] C. H. Lampert, H. Nickisch, and S. Harmeling. “Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer”. In CVPR, 2009<br>[3] C. H. Lampert, H. Nickisch, and S. Harmeling. “Attribute-Based Classification for Zero-Shot Visual Object Categorization”. IEEE T-PAMI, 2013</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/刘唯_动物属性标记数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/刘唯_动物属性标记数据集/" itemprop="url">动物属性标记数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘维<br>下载地址：<a href="http://cvml.ist.ac.at/AwA2/" target="_blank" rel="noopener">http://cvml.ist.ac.at/AwA2/</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>该数据集提供了一个平台，用于基准的转移学习算法，特别是属性基分类和零射学习。它可以充当原始动物的替代，使用属性(AwA)数据集，因为它具有相同的类结构和几乎相同的特征。它包含了37322个50个动物的图像，每个图像都有预先提取的特征表示。这些类与Osherson的经典类/属性矩阵一致，从而为每个类提供85个数字属性值。使用共享属性，可以在不同的类之间传递信息。这些图像数据是在2016年从Flickr等公共资源中收集的。</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p> 大小：包含了37322个50个动物的图像。<br> 1.CUHK student data set 含188张faces<br> 2.AR data set (123 faces)<br> 3.XM2VTS data set (295 faces)</p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Y. Xian, C. H. Lampert, B. Schiele, Z. Akata. “Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly” arXiv:1707.00600</p>
<p>[2] C. H. Lampert, H. Nickisch, and S. Harmeling. “Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer”. In CVPR, 2009<br>[3] C. H. Lampert, H. Nickisch, and S. Harmeling. “Attribute-Based Classification for Zero-Shot Visual Object Categorization”. IEEE T-PAMI, 2013<br>[4]X. Tang, and X. Wang, “Face Photo Recognition Using Sketch,” in Proceedings of IEEE International Conference on Image Processing (ICIP), Vol. 1, pp. 257-260, Rochester, New York, Sept. 2002.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/刘唯_人脸素描数据集 /">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/刘唯_人脸素描数据集 /" itemprop="url">人脸素描数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘维<br>下载地址：<a href="http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html" target="_blank" rel="noopener">http://mmlab.ie.cuhk.edu.hk/archive/facesketch.html</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>中大脸部素描数据库(CUFS)是面向人脸素描合成和人脸素描识别的研究。它包括来自香港中文大学(中大)学生数据库的188张脸，来自AR数据库的123张脸，以及295张来自XM2VTS数据库的面孔。总共有606张脸。对于每张脸，都有一幅画是由一位艺术家绘制的。</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p> 大小：包含3个文件数据文件。<br> 1.CUHK student data set 含188张faces<br> 2.AR data set (123 faces)<br> 3.XM2VTS data set (295 faces)</p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><ol>
<li><p>X. Wang and X. Tang, “Face Photo-Sketch Synthesis and Recognition,” IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), Vol. 31, 2009.</p>
</li>
<li><p>Qingshan Liu, Xiaoou Tang, Hongliang Jin, Hanqing Lu, and Songde Ma,  A Nonlinear Approach For Face Sketch Synthesis and Recognition,  Int’l Conf. on Computer Vision and Pattern Recognition (CVPR), 2005.</p>
</li>
<li><p>X. Tang, and X. Wang, “Face Sketch Recognition,” IEEE Transactions on Circuits and Systems for Video Technology (CSVT), Special Issue on Image- and Video- Based Biometrics, Vol. 14, No. 1, pp. 50-57, January, 2004.</p>
</li>
<li><p>X. Tang, and X. Wang, “Face Sketch Synthesis and Recognition,” in Proceedings of IEEE International Conference on Computer Vision (ICCV), 2003.</p>
</li>
<li><p>X. Tang, and X. Wang, “Face Photo Recognition Using Sketch,” in Proceedings of IEEE International Conference on Image Processing (ICIP), Vol. 1, pp. 257-260, Rochester, New York, Sept. 2002.</p>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/27/卢梦依_语义关系分类数据集-semeval2007Task4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/27/卢梦依_语义关系分类数据集-semeval2007Task4/" itemprop="url">语义关系分类数据集-semeval2007Task4</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-27T21:47:46+05:00">
                2018-05-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依<br>下载地址：<a href="https://github.com/davidsbatista/Annotated-Semantic-Relationships-Datasets/blob/master/datasets/SemEval2007-Task4.tar.gz" target="_blank" rel="noopener">https://github.com/davidsbatista/Annotated-Semantic-Relationships-Datasets/blob/master/datasets/SemEval2007-Task4.tar.gz</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h2 id="数据集概述"><a href="#数据集概述" class="headerlink" title="数据集概述"></a>数据集概述</h2><p>Task 4的主要任务是简单名词(名词或基本名词短语)之间的语义关系的分类，例如，蜜蜂，显示了产品生产者关系的一个实例。这种分类发生在书面英语文本中的一个句子的语境中。语义关系分类算法可以应用于信息检索、信息提取、文本摘要、问答等方面。对文本蕴涵(Tatu和Moldovan, 2005)的认识是在高端NLP应用中成功使用这种类型的深入分析的一个例子。</p>
<h1 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h1><p> 大小：小数据集，包含7个关系类型和总共1529个注释示例。</p>
<p> 示例：<br><img src="https://i.loli.net/2018/05/27/5b09c465cdde2.jpg" alt=""></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>1.T. Chklovskiand P. Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Proc.Conf.onEmpiricalMethodsin NaturalLanguageProcessing, EMNLP-04, pages 33–40, Barcelona, Spain.<br>2.R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19:479–496.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/05/25/朱述承_中研院中文句结构树资料库/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/25/朱述承_中研院中文句结构树资料库/" itemprop="url">中研院中文句结构树资料库</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-25T15:37:46+05:00">
                2018-05-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：朱述承<br>访问地址：<a href="http://treebank.sinica.edu.tw/" target="_blank" rel="noopener">http://treebank.sinica.edu.tw/</a></p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>“中文句结构树资料库”(Sinica Treebank Version 3.0) 包含了6个档案，61,087个中文树图，361,834个词，是中央研究院词库小组从中央研究院平衡语料库 (Sinica Corpus) 中抽取句子，经由电脑剖析成结构树，并加以人工修正、检验后所得的成果。在中文句结构树中，我们标示了中文句语意和语法的讯息。此一“中文句结构树资料库”目前开放网上检索及资料移转，以供学者专家在中文句法、语意关系研究参考之用。另有1000个句结构树开放下载。</p>
<h1 id="100棵树图参考资料"><a href="#100棵树图参考资料" class="headerlink" title="100棵树图参考资料"></a>100棵树图参考资料</h1><p><a href="http://turing.iis.sinica.edu.tw/treesearch/" target="_blank" rel="noopener">http://turing.iis.sinica.edu.tw/treesearch/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/22/">22</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">CNLR</p>
              <p class="site-description motion-element" itemprop="description">语料库、数据集及工具资源和教程</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">214</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CNLR</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
