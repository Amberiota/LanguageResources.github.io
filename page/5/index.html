<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="语料库、数据集及工具资源和教程">
<meta property="og:type" content="website">
<meta property="og:title" content="世界语言资源平台">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name" content="世界语言资源平台">
<meta property="og:description" content="语料库、数据集及工具资源和教程">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="世界语言资源平台">
<meta name="twitter:description" content="语料库、数据集及工具资源和教程">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/5/"/>





  <title>世界语言资源平台</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">世界语言资源平台</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/卢梦依_SVHN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/卢梦依_SVHN/" itemprop="url">SVHN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T21:23:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>SVHN数据来源于 Google 街景视图中房屋信息，它是一个真实世界的图像数据集，用于开发机器学习和对象识别算法，对数据预处理和格式化的要求最低。它跟MNIST相似，但是包含更多数量级的标签数据（超过60万个数字图像），并且来源更加多样，用来识别自然场景图像中的数字。</p>
<h1 id="地址"><a href="#地址" class="headerlink" title="地址"></a>地址</h1><p><a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" rel="noopener">http://ufldl.stanford.edu/housenumbers/</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]Shuai Li,Wenfeng Song,Hong Qin,Aimin Hao. Deep variance network: An iterative, improved CNN framework for unbalanced training datasets[J]. Pattern Recognition,2018,81.<br>[2]Andrey V. Savchenko,Natalya S. Belova. Unconstrained face identification using maximum likelihood of distances between deep off-the-shelf features[J]. Expert Systems With Applications,2018,108.<br>[3]Alistair Peter McGeorge. An Urban Partnership for Inner Sydney Social Inclusion, Health and Well-being[J]. International Journal of Integrated Care,2017,17(3).</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/卢梦依_Labeled Faces in the Wild数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/卢梦依_Labeled Faces in the Wild数据集/" itemprop="url">Labeled Faces in the Wild数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T21:19:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>该数据集是用于研究无约束面部识别问题的面部照片数据库。数据集包含从网络收集的13000多张图像。每张脸都贴上了所画的人的名字，图片中的1680人在数据集中有两个或更多不同的照片。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p><a href="http://vis-www.cs.umass.edu/lfw/" target="_blank" rel="noopener">http://vis-www.cs.umass.edu/lfw/</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]David Rim,Md Kamrul Hasan,Fannie Puech,Christopher J. Pal. Learning from weakly labeled faces and video in the wild[J]. Pattern Recognition,2015,48(3).<br>[2]Davide Lombardo. An explicit open image theorem for products of elliptic curves[J]. Journal of Number Theory,2016,168.<br>[3]M. Nazir,A. Majid-Mirza,S. Ali-Khan. PSO-GA Based Optimized Feature Selection Using Facial and Clothing Information for Gender Classification[J]. Journal of Applied Research and Technology,2014,12(1).<br>[4]Jiang-Jing Lv,Cheng Cheng,Guo-Dong Tian,Xiang-Dong Zhou,Xi Zhou. Landmark perturbation-based data augmentation for unconstrained face recognition[J]. Signal Processing: Image Communication,2016,47.<br>[5]Blondin , John M.,Kallman , Timothy R.,Pereyra , Nicolas Antonio. Hydrodynamic Models of Line-Driven Accretion Disk Winds in Cataclysmic Variables[J]. Revista Mexicana de Astronomía y Astrofísica : Universidad Nacional Autónoma de México. Instituto de Astronomía,2001(11).<br>[6]Ian W. Roxburgh. Challenges to Theories of the Structure of Moderate-Mass Stars[M].Springer Berlin Heidelberg:2005-07-19.<br>[7]Andrey V. Savchenko,Natalya S. Belova. Unconstrained face identification using maximum likelihood of distances between deep off-the-shelf features[J]. Expert Systems With Applications,2018,108. </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/卢梦依_LSUN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/卢梦依_LSUN/" itemprop="url">LSUN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T21:14:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：卢梦依</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>国外的PASCAL VOC和ImageNet ILSVRC比赛使用的数据集，数据领域包括卧室、冰箱、教师、厨房、起居室、酒店等多个主题。</p>
<h1 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h1><p><a href="http://lsun.cs.princeton.edu/2017/" target="_blank" rel="noopener">http://lsun.cs.princeton.edu/2017/</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]Blondin , John M.,Kallman , Timothy R.,Pereyra , Nicolas Antonio. Hydrodynamic Models of Line-Driven Accretion Disk Winds in Cataclysmic Variables[J]. Revista Mexicana de Astronomía y Astrofísica : Universidad Nacional Autónoma de México. Instituto de Astronomía,2001(11).<br>[2]Ian W. Roxburgh. Challenges to Theories of the Structure of Moderate-Mass Stars[M].Springer Berlin Heidelberg:2005-07-19.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘唯_Labelme/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘唯_Labelme/" itemprop="url">Labelme</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T20:10:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘唯</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p> Labelme是斯坦福一个学生的母亲利用休息时间帮儿子做的标注，后来便发展为一个数据集。该数据集的主要特点包括<br>（1）专门为物体分类识别设计，而非仅仅是实例识别<br>（2）专门为学习嵌入在一个场景中的对象而设计<br>（3）高质量的像素级别标注，包括多边形框（polygons）和背景标注（segmentation masks）<br>（4）物体类别多样性大，每种物体的差异性，多样性也大。<br>（5）所有图像都是自己通过相机拍摄，而非copy<br>（6）公开的，免费的</p>
<h1 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h1><p>图像如下图所示，需要通过matlab来下载，一种奇特的下载方式。<br>下载链接为<a href="http://labelme2.csail.mit.edu/Release3.0/index.php" target="_blank" rel="noopener">http://labelme2.csail.mit.edu/Release3.0/index.php</a><br><img src="https://i.loli.net/2018/05/24/5b06ac87bd99e.jpg" alt=""></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]吉江燕,方挺.基于Labelme的参考图像的手工分割[J].微型机与应用,2015,34(17):49-51+56.<br>[2]Bryan C. Russell,  Antonio Torralba,  Kevin P. Murphy,  William T. Freeman.International Journal of Computer Vision[J].LabelMe: A Database and Web-Based Tool for Image Annotation.<br>[3]</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘唯_Open Image/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘唯_Open Image/" itemprop="url">Open Image</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T20:10:00+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘唯</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>过去几年机器学习的发展使得计算机视觉有了快速的进步，系统能够自动描述图片，对共享的图片创造自然语言回应。其中大部分的进展都可归因于 ImageNet 、COCO这样的数据集的公开使用。谷歌作为一家伟大的公司，自然也要做出些表示，于是乎就有了Open Image。</p>
<p>Open Image是一个包含~900万张图像URL的数据集，里面的图片通过标签注释被分为6000多类。该数据集中的标签要比ImageNet（1000类）包含更真实生活的实体存在，它足够让我们从头开始训练深度神经网络。</p>
<p>谷歌出品，必属精品！唯一不足的可能就是它只是提供图片URL，使用起来可能不如直接提供图片方便。</p>
<h1 id="数据集大小"><a href="#数据集大小" class="headerlink" title="数据集大小"></a>数据集大小</h1><p>~1.5GB（不包括图片）</p>
<h1 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h1><p><a href="https://github.com/openimages/dataset" target="_blank" rel="noopener">https://github.com/openimages/dataset</a></p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1]M. Hu&amp;scaron,ek. Open images of orderable spaces[J]. proc,1983,88(4).<br>[2]Davide Lombardo. An explicit open image theorem for products of elliptic curves[J]. Journal of Number Theory,2016,168.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_UCF101 - Action Recognition Data Set/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_UCF101 - Action Recognition Data Set/" itemprop="url">UCF101 - Action Recognition Data Set</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://crcv.ucf.edu/data/UCF101.php" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101.php</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>UCF101是一个动作识别数据集，包含现实的动作视频，从YouTube上收集，有101个动作类别。该数据集是UCF50数据集的扩展，该数据集有50个动作类别。<br>从101个动作类的13320个视频中，UCF101给出了最大的多样性，并且在摄像机运动、物体外观和姿态、物体尺度、视点、杂乱背景、光照条件等方面存在较大的差异，这是迄今为止最具挑战性的数据。<br>由于大多数可用的动作识别数据集都不现实，而且是由参与者进行的，UCF101旨在通过学习和探索新的现实行动类别来鼓励进一步研究行动识别。<br>101个动作类的视频被分成25组，每个组可以包含4-7个动作视频。同一组的视频可能有一些共同的特点，比如背景相似、观点相似等。  </p>
<p><strong>动作类别可以分为五类： </strong> </p>
<ul>
<li>Human-Object Interaction  </li>
<li>Body-Motion Only   </li>
<li>Human-Human Interaction   </li>
<li>Playing Musical Instruments   </li>
<li>Sports.   </li>
</ul>
<p>UCF101数据集的操作类别为:应用眼妆、唇膏、射箭、婴儿爬行,平衡木,乐队游行,棒球,篮球,篮球扣篮,卧推,骑自行车,台球,吹干头发,吹蜡烛,体重下蹲,保龄球,拳击出气筒,拳击袋速度,蛙泳,刷牙,挺举,悬崖跳水,板球保龄球,板球,削减在厨房,潜水,打鼓,击剑、曲棍球点球,地板体操,飞盘,爬泳,高尔夫挥杆,发型、链球、锤击,倒立俯卧撑,倒立行走,头部按摩,跳高,赛马,骑马、呼啦圈、冰上舞蹈,掷标枪,杂耍球,跳绳,跳杰克,皮划艇,针织,跳远,弓步,阅兵,搅拌面糊、拖地板,修女轻叩,双杠,披萨扔,弹吉他,弹钢琴,打手鼓,演奏小提琴,演奏大提琴,玩来说,玩时代,演奏长笛,玩锡塔尔琴,撑杆跳,鞍马、拉Ups、穿孔、俯卧撑、漂流、攀岩室内,绳索攀爬、划船、萨尔萨舞旋转,剃胡子,推铅球,滑板,滑雪,Skijet,跳伞,足球杂耍,足球点球,还是戒指,相扑,冲浪,秋千,乒乓球拍、太极、网球挥拍,扔铁饼,蹦床跳,打字,高低杠,排球飙升,与狗一起散步,“推墙”,写作,溜溜球。  </p>
<p>下载UCF101数据集：<a href="http://crcv.ucf.edu/data/UCF101/UCF101.rar" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101.rar</a>  </p>
<p>UCF101数据集的动作识别（ Action Recognition）的训练/测试集下载地址:<a href="http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip</a>  </p>
<p>UCF101数据集的动作检测（ Action Detection）的训练/测试集下载地址:<a href="http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-DetectionTask.zip" target="_blank" rel="noopener">http://crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-DetectionTask.zip</a>  </p>
<p>UCF101数据集的STIP特性可以在这里下载:<a href="http://crcv.ucf.edu/data/UCF101/UCF101_STIP_Part1.rar" target="_blank" rel="noopener">Part1</a>,<a href="http://crcv.ucf.edu/data/UCF101/UCF101_STIP_Part2.rar" target="_blank" rel="noopener">Part2</a>  </p>
<p><img src="https://i.loli.net/2018/06/02/5b121da3d8ad6.jpg" alt="">  </p>
<h1 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h1><p><img src="http://crcv.ucf.edu/data/UCF101/Clip%20Duration%201.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Clip%20Duration%202.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Number%20of%20Videos%201.jpg" alt="">  </p>
<p><img src="http://crcv.ucf.edu/data/UCF101/Number%20of%20Videos%202.jpg" alt="">  </p>
<p>注意：将属于同一组的视频保持在训练和测试中非常重要。由于一组中的视频是从单个长视频中获得的，因此在训练和测试套件中共享来自同一组的视频会获得较高的性能。 </p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Khurram Soomro, Amir Roshan Zamir and Mubarak Shah, UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild, CRCV-TR-12-01, November, 2012.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_百度BROAD-Video Highlights视频精彩片段数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_百度BROAD-Video Highlights视频精彩片段数据集/" itemprop="url">百度BROAD-Video Highlights视频精彩片段数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://ai.baidu.com/broad" target="_blank" rel="noopener">http://ai.baidu.com/broad</a></p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>该数据集（下文中简称为BROAD-VH数据集）在介绍中将其定义为视频精彩片段提取任务。具体而言，就是提取视频中可能包含精彩片段的时间区域，而不需要对精彩片段的类别进行分类。该任务实际上与我之前介绍过的temporal action proposal 任务（相关介绍和算法可以参考<a href="https://zhuanlan.zhihu.com/p/31501316" target="_blank" rel="noopener">Temporal Action Detection (时序动作检测)方向2017年会议论文整理）</a>是完全相同的。</p>
<h1 id="视频及数据信息"><a href="#视频及数据信息" class="headerlink" title="视频及数据信息"></a>视频及数据信息</h1><p>BROAD-VH数据集主要来源于爱奇艺视频。视频类型为综艺节目，目前包括1500个长视频，视频总时长约1200小时。该数据集的视频时长分布图如下图所示（长度单位为帧）。按照总帧数和总时长的比例，估计采样的fps大概为1.5吧，算是比较低的采样频率了。</p>
<p>训练/验证/测试集的视频数量划分为1262/120/117。</p>
<p>该数据集通过爱奇艺网页link的方式提供了原始视频（即需要爬虫下载或手动下载），此外还提供了提取好的image feature和audio feature。这两种特征均在视频的每一帧上提取，维度均为2048。比如对于一个长度为1000帧的视频，image和audio特征矩阵的大小均为1000*2048。</p>
<h1 id="标签信息及分布"><a href="#标签信息及分布" class="headerlink" title="标签信息及分布"></a>标签信息及分布</h1><p>该数据集中一共有18000个精彩片段的时序标注，平均一个视频有12个时序标注。这些精彩片段的总时长占1500个小时中的750个小时，即有一半左右的视频时长被标注为了精彩片段。</p>
<p>我对训练集的标签信息进行了分析，分析的主要内容为精彩片段时长的分布，分布直方图如下图所示。</p>
<p>可以看出，大部分精彩片段的长度都在30-300帧的范围。</p>
<h1 id="测评方式"><a href="#测评方式" class="headerlink" title="测评方式"></a>测评方式</h1><p>测评方式部分与通常temporal action proposal任务中不同，并没有使用average recall (平均召回率），而是同detection任务一样使用了mAP，此处将所有highlights片段都看作为了一个动作类别。比较有趣的是，BROAD-VH基本上直接使用了ActivityNet Challenge的detection任务测评代码（略有改动）。</p>
<h1 id="简要分析"><a href="#简要分析" class="headerlink" title="简要分析"></a>简要分析</h1><p>根据上面的介绍以及分析内容，可以对这个数据集进行一些简单的评价：</p>
<ul>
<li>单个视频的时长可能很长（小时级别），单个视频中包含的精彩片段也比较多，这点与THUMOS数据集很像，而与单个视频时长短且包含片段少的ActivityNet数据集差异大</li>
<li>数据集标注的格式，测评代码等方面应该是直接参考的ActivityNet 数据集做的</li>
<li>数据的规模还是比较大的，从时长方面看比ActivityNet要长（ActivityNet时长大约为700小时）</li>
<li>视频的来源均为综艺视频，这点表明这个数据集的来源多样性比较单一</li>
<li>提供特征，其目的应该是节省研究者的计算开销。估计1500小时的视频，提取一遍特需要很长的时间。。根本没法玩。所以有现成的特征挺不错的。</li>
</ul>
<h1 id="简单的尝试"><a href="#简单的尝试" class="headerlink" title="简单的尝试"></a>简单的尝试</h1><p>下完数据集我就先跑了一个最简单的baseline方法，即activitynet challenge 2017 proposal task中的baseline：uniform random 方法。代码主要参考了activitynet官方提供的代码：<a href="https://link.zhihu.com/?target=https%3A//github.com/activitynet/ActivityNet/blob/master/Notebooks/ActivityNet-Release1.3.Proposals.ipynb" target="_blank" rel="noopener">activitynet/proposals</a></p>
<p>简单而言，就是在视频随机的位置产生随机长度的proposals，并给予随机的confidence score。在验证集中，对于每个视频我生成了200个proposals，得到的mAP大概在0.027 左右。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_CoPhIR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_CoPhIR/" itemprop="url">CoPhIR 数据集</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓<br>地址：<a href="http://cophir.isti.cnr.it/whatis.html" target="_blank" rel="noopener">http://cophir.isti.cnr.it/whatis.html</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>雅虎发布的超大Flickr数据集，包含1亿多张图片。<br>CoPhIR（Content-based Photo Image Retrieval，基于内容的照片图像检索）测试集合的开发旨在对SAPIR项目基础设施（SAPIR：使用对等IR中的音频视频内容进行搜索）的可扩展性进行重要测试以进行相似性搜索。 CoPhIR是NMIS实验室与意大利比萨ISTI-CNR的HPC实验室共同努力的成果。 我们通过DILIGENT项目使用EGEE European GRID从Flickr存档提取元数据。 对于每个图像，已经提取了标准的MPEG-7图像特征。试验台的每个入口都包含：  </p>
<ul>
<li>链接Flickr网站的相应条目   </li>
<li>照片图像缩略图  </li>
<li>一个XML结构，在相应的Flickr条目中包含Flickr用户信息：标题，位置，GPS，标签，注释等。   </li>
<li><p>具有5个提取的标准MPEG-7图像特征的XML结构：  </p>
<ul>
<li>可伸缩的色彩  </li>
<li>色彩结构 </li>
<li>色彩布局  </li>
<li>边缘直方图</li>
<li>均匀纹理  </li>
</ul>
</li>
</ul>
<p>迄今收集的数据代表世界上最大的多媒体元数据收集，可用于可扩展相似性搜索技术的研究。 CoPhIR包含1.06亿个处理过的图像。   </p>
<p>CoPhIR现在可供研究人员尝试比较不同的索引技术进行相似性搜索，其中可扩展性是关键问题。  </p>
<p>我们使用Flickr图片内容符合Creative Commons许可。 CoPhIR测试集合符合基于WIPO（世界知识产权组织）版权条约和表演和录音制品条约以及意大利现行法律68/2003的欧洲第29/2001号建议书。<br>为了访问CoPhIR发行版，有兴趣在其上进行实验的组织（大学，研究实验室等）将必须签署随附的CoPhIR访问协议和CoPhIR访问注册表，将原始签名文件通过邮件发送给我们。请按照“如何获得CoPhIR测试集合”一节中的说明进行操作。然后，您将收到登录和密码以下载所需的文件。  </p>
<h1 id="使用–获得CoPhIR测试集"><a href="#使用–获得CoPhIR测试集" class="headerlink" title="使用–获得CoPhIR测试集"></a>使用–获得CoPhIR测试集</h1><ul>
<li>发送电子邮件到 cophiristi.cnr.it (subject: new access to Cophir)，包含有必要信息的请求(见<a href="http://cophir.isti.cnr.it/RequestTemplate.txt" target="_blank" rel="noopener">请求模板</a>)。  </li>
<li>打印CoPhIR Access Agreement和CoPhIR Access Registration Form (doc, pdf)，填写所需信息，然后由授权人签署正本文件。  </li>
<li>将两份文件邮寄至</li>
</ul>
<blockquote>
<p>Dr. Fausto Rabitti<br>NMIS Lab.<br>ISTI-CNR, Pisa Research Area<br>Via Moruzzi, 1<br>56124 Pisa (Italy).  </p>
</blockquote>
<ul>
<li><p>我们将发送到您的电子邮件地址，在访问注册表中显示，一封包含登录名和密码的电子邮件将用于访问CoPhIR测试集合。  </p>
</li>
<li><p>要下载CoPhIR测试集合的文件，请在CoPhIR网站上输入<a href="http://cophir.isti.cnr.it/download.html" target="_blank" rel="noopener">下载</a>部分并使用您的登录名和密码。  </p>
</li>
</ul>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] F Rabitti, R Perego,F Falchi,C Lucchese, P Bolettieri, CoPhIR (Content-based Photo Image Retrieval) Test-Collection, 2008<br>[2] M Batko,P Kohoutkova,D Novak, CoPhIR Image Collection under the Microscope, 2009  </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_AVA（atomic visual actions）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_AVA（atomic visual actions）/" itemprop="url">AVA（atomic visual actions）</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="http://research.google.com/ava/" target="_blank" rel="noopener">http://research.google.com/ava/</a>  </p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>教机器理解视频中的人的行为是计算机视觉中的一个基本研究问题，谷歌blog发布了一个堪比“视频版”ImageNet的数据集-AVA（A Finely Labeled Video Dataset for Human Action Understanding ），旨在教机器理解人的活动。 该数据集以人类为中心进行标注，包含80类动作的 57600 个视频片段，有助于人类行为识别系统的研究。
　　  </p>
<p>教机器理解视频中的人的行为是计算机视觉中的一个基本研究问题，对个人视频搜索和发现、运动分析和手势界面等应用十分重要。尽管在过去的几年里，对图像进行分类和在图像中寻找目标对象方面取得了令人兴奋的突破，但识别人类的动作仍然是一个巨大的挑战。这是因为动作的定义比视频中的对象的定义要差，因此很难构造一个精细标记的动作视频数据集。许多基准数据集，例如 UCF101、activitynet 和DeepMind 的 Kinetics，都是采用图像分类的标记方案，在数据集中为每个视频或视频片段分配一个标签，而没有数据集能用于包含多个可能执行不同动作的人的复杂场景。</p>
<p>谷歌上周发布一个新的电影片段数据集，旨在教机器理解人的活动。这个数据集被称为 AVA（atomic visual action），这些视频对人类来说并不是很特别的东西——仅仅是 YouTube 上人们喝水、做饭等等的3秒钟视频片段。但每段视频都与一个文件捆绑在一起，这个文件勾勒了机器学习算法应该观察的人，描述他们的姿势，以及他们是否正在与另一个人或物进行互动。就像指着一只狗狗给一个小孩看，并教他说“狗！”，这个数据集是这类场景的数字版本。</p>
<h1 id="数据集特点"><a href="#数据集特点" class="headerlink" title="数据集特点"></a>数据集特点</h1><p>相比其他的动作数据集，AVA数据集有以下这些特点：</p>
<p><strong>以人为中心进行标注</strong>：每个动作标签都基于人物本身，而不是一段视频或者剪辑片段。因此，我们能够为不同动作中的各类人加上不同的标签，这一点非常常见。</p>
<p><strong>原子级视觉动作</strong>：我们对需要标注的动作进行了合理的时间限制（3秒钟），以确保动作符合人的生理机能，同时有明显的视觉特征。</p>
<p><strong>真实视频作为视觉材料</strong>：我们使用不同题材和国家的电影作为AVA的标注材料，进而确保数据库中包含各类型的人类行为。  </p>
<p><img src="http://ss.csdn.net/p?https://mmbiz.qpic.cn/mmbiz_gif/wc7YNPm3YxVcUeMuMHYKbUFVF2ibGkUgw490HgjQfU5kN8NVYbPBNbJgZR9pJ1BN8Mc5iaPIdAicKLtwmsONEDvYg/0?wx_fmt=gif" alt="">  </p>
<p>视频来源中的3秒视觉片段标签，用方框标注出每个动作素材（为确保清晰，每个例子中只出现了一个框。）</p>
<p>为创建 AVA，我们首先从 YouTube 上收集了大量多样化的数据，主要集中在「电影」和「电视」类别，选择来自不同国家的专业演员。我们对每个视频抽取 15 分钟进行分析，并统一将 15 分钟视频分割成 300 个非重叠的 3 秒片段。采样遵循保持动作序列的时间顺序这一策略。</p>
<p>接下来，我们为每个 3 秒片段中间帧的人物手动标注边界框。对标注框中的每个人，标注者从预制的原子动作词汇表（80 个类别）中选择适当数量的标签来描述人物动作。这些动作可分为三组：姿势／移动动作、人-物互动和人-人互动。我们对执行动作的所有人进行了全部标注，因此 AVA 的标签频率遵循长尾分布，如下图所示。 </p>
<p><img src="http://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxVcUeMuMHYKbUFVF2ibGkUgwH3wDtoVY30WySELT8FopyRb1C7X31BoDib7OKBmKVDa2CDlx8HUtkQQ/0?wx_fmt=png" alt="">  </p>
<p>AVA 的原子动作标签分布。x 轴所示标签只是词汇表的一部分。</p>
<p>AVA 的独特设计使我们能够获取其他现有数据集中所没有的一些有趣数据。例如，给出大量至少带有两个标签的人物，我们可以判断动作标签的共现模式（co-occurrence pattern）。下图显示 AVA 中共现频率最高的动作对及其共现得分。我们确定的期望模式有：人们边唱歌边弹奏乐器、拥吻等。<br> <img src="http://ss.csdn.net/p?http://mmbiz.qpic.cn/mmbiz_png/wc7YNPm3YxVcUeMuMHYKbUFVF2ibGkUgwsojISBHw3F5O3pGhnDIDB6MNXe72falLbIWcbicgCdQhEIULVlSmZ1A/0?wx_fmt=png" alt=""><br>AVA 中共现频率最高的动作对。</p>
<p>为评估基于 AVA 数据集的人类动作识别系统的高效性，我们使用一个现有的基线深度学习模型在规模稍小一些的 JHMDB dataset 上取得了具备高竞争性的性能。由于存在可变焦距、背景杂乱、摄影和外观的不同情况，该模型在 JHMDB dataset 上的性能与在 AVA 上准确识别动作的性能（18.4% mAP）相比稍差。这表明，未来 AVA 可以作为开发和评估新的动作识别架构和算法的测试平台。  </p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik, AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions, 2017</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/02/刘晓_Kinetics-600 dataset/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="CNLR">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="世界语言资源平台">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/02/刘晓_Kinetics-600 dataset/" itemprop="url">Kinetics-600 dataset</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-02T10:31:58+05:00">
                2018-06-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>提供者：刘晓</p>
<p>地址：<a href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/" target="_blank" rel="noopener">https://deepmind.com/research/open-source/open-source-datasets/kinetics/</a>  </p>
<h1 id="背景与来源"><a href="#背景与来源" class="headerlink" title="背景与来源"></a>背景与来源</h1><p>目前activitynet第三届比赛已经开始了，这项比赛始于2016CVPR，是与ImageNet齐名的在视频理解方面最重要的比赛。</p>
<p>在这个比赛下的Task A – Trimmed Action Recognition比赛是一个视频分类比赛，数据集就是kinetics-600数据集。数据集有Google的deepmind团队提供，2017年是第一届比赛，当时有400个类别，20多万数据，今年又对数据集进行了扩增，现在有600个类别，共50万左右的视频。</p>
<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>视频来源于YouTube，一共有600个类别，每个类别至少600个视频以上，每段视频持续10秒左右。类别主要分为三大类：人与物互动，比如演奏乐器；人人互动，比如握手、拥抱；运动等。即person、person-person、person-object。</p>
<h1 id="论文介绍"><a href="#论文介绍" class="headerlink" title="论文介绍"></a>论文介绍</h1><h2 id="数据集类别介绍"><a href="#数据集类别介绍" class="headerlink" title="数据集类别介绍"></a>数据集类别介绍</h2><p>下面这些介绍主要是参考于17年deepmind发的论文”The Kinetics Human Action Video Dataset”，当时还是400类的数据集。论文请见。</p>
<p>数据集主要关注人类行为，action类的列表包括：</p>
<p>单人行为，例如绘画，喝酒，大笑，抽拳;</p>
<p>人人行为，例如拥抱，亲吻，握手；</p>
<p>人物行为，例如打开礼物，修剪草坪，洗碗。</p>
<p>一些行动是比较细粒度的，需要时序推理来区分，例如，不同类型的游泳。其他动作类别需要强调区分对象，例如演奏不同类型的乐器。</p>
<p>这些类别并没有严格的层级划分，但是还是有些的，类似父类子类关系，例如音乐类（打鼓、长号、小提琴、……），个人卫生类（刷牙、剪指甲、洗手、……），跳舞类（芭蕾、macarena、tap、……），烹饪(切割、煎、脱皮、……）等。</p>
<p>数据集有400个类别，每个动作都有400-1150个视频片段，每段视频的时长都在10秒左右。目前的版本有306245视频，分为三个部分，训练时每个类为250-1000个视频，验证时每个类50个视频，测试时每个类100个视频。</p>
<p>每个类都包含了一种行动。但是，一个特定的剪辑可以包含<br>几种动作。例如，开车”时“发短信”;“弹奏尤克里里”时“跳草裙舞”;“跳舞”时“刷牙”。这种情况下，这个视频只会标记一个标签，并不会同时存在于两个类种。因此，取top-5的准确率更为合适。</p>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>一共验证了三种处理行为识别的主流模型，分别是LSTM、two stream和3dcnn。<br><img src="https://i.loli.net/2018/06/02/5b12089d68a3f.jpg" alt="">   </p>
<p>放上准确率，可以看出two-stream还是占据主导优势的。在kinetics数据集上，top-1是61.0，top-5是81.3。<br><img src="https://i.loli.net/2018/06/02/5b1208bda6f55.jpg" alt="">  </p>
<h2 id="分类结果分析"><a href="#分类结果分析" class="headerlink" title="分类结果分析"></a>分类结果分析</h2><p><img src="https://i.loli.net/2018/06/02/5b1208f97135e.jpg" alt="">  </p>
<p>上图是那些类分的比较好，那些类分的比较差。</p>
<p>还有一点，由于很多动作比较细粒度，类别之间容易造成混淆，论文中也给出了最容易混淆的几个类别，比如，跳远和三级跳远，吃汉堡和吃甜甜圈。swing跳舞和跳萨尔萨舞等都会混淆。</p>
<p>如下图：<br><img src="https://i.loli.net/2018/06/02/5b12091f8f6b5.jpg" alt="">  </p>
<p>当然，由于使用two stream模型，光流模型和RGB模型可能对不同的动作有不同的准确度，对于这些特定类别，可以在融合时对光流和RGB设定不同的权重。<br><img src="https://i.loli.net/2018/06/02/5b12094bd1965.jpg" alt="">  </p>
<h2 id="粒度划分"><a href="#粒度划分" class="headerlink" title="粒度划分"></a>粒度划分</h2><p>最后给出这些类别的一个粒度划分，可能有多个类别都会属于同一大类。即父类子类关系。</p>
<p>首先列出有哪些父类，然后再给出每个父类下的子类。</p>
<h3 id="父类："><a href="#父类：" class="headerlink" title="父类："></a>父类：</h3><p>共38大类，每个类后面的数字代表有几个子类。</p>
<p><img src="https://i.loli.net/2018/06/02/5b1209d4606a5.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b1209f3b13d7.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a1262d07.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a3884cf5.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a62ee102.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120a828fe0e.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120ac63f9b1.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120af45341c.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b18ecaa3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b2b19ba4.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b497dbb3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120b8f3a289.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120bb8b3fb7.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120bdac4ade.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c069dbda.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c250c117.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c3d062f0.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c61e5fa3.jpg" alt=""><br><img src="https://i.loli.net/2018/06/02/5b120c7f966eb.jpg" alt="">  </p>
<h1 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h1><p>[1] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman, The Kinetics Human Action Video Dataset, 2017</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/20/">20</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">CNLR</p>
              <p class="site-description motion-element" itemprop="description">语料库、数据集及工具资源和教程</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">192</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">CNLR</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
